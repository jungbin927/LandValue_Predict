{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "13a978b3",
      "metadata": {
        "id": "13a978b3"
      },
      "source": [
        "## VIF ë¯¸ì œê±°"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "49a0bb68",
      "metadata": {
        "id": "49a0bb68"
      },
      "source": [
        "### ì½”ë© ë‹¤ìš´ë¡œë“œ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "EKkY3Eb27eEm",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EKkY3Eb27eEm",
        "outputId": "facc8fad-2284-4fb7-be15-be164950965e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting numpy<2.0\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[?25l     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/61.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m80.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.26.4\n",
            "Collecting rtdl\n",
            "  Downloading rtdl-0.0.13-py3-none-any.whl.metadata (1.0 kB)\n",
            "Requirement already satisfied: numpy<2,>=1.18 in /usr/local/lib/python3.11/dist-packages (from rtdl) (1.26.4)\n",
            "Collecting torch<2,>=1.7 (from rtdl)\n",
            "  Downloading torch-1.13.1-cp311-cp311-manylinux1_x86_64.whl.metadata (24 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch<2,>=1.7->rtdl) (4.14.0)\n",
            "Collecting nvidia-cuda-runtime-cu11==11.7.99 (from torch<2,>=1.7->rtdl)\n",
            "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu11==8.5.0.96 (from torch<2,>=1.7->rtdl)\n",
            "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu11==11.10.3.66 (from torch<2,>=1.7->rtdl)\n",
            "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch<2,>=1.7->rtdl)\n",
            "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch<2,>=1.7->rtdl) (75.2.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch<2,>=1.7->rtdl) (0.45.1)\n",
            "Downloading rtdl-0.0.13-py3-none-any.whl (23 kB)\n",
            "Downloading torch-1.13.1-cp311-cp311-manylinux1_x86_64.whl (887.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m887.4/887.4 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m81.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m58.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cublas-cu11, nvidia-cudnn-cu11, torch, rtdl\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.6.0+cu124\n",
            "    Uninstalling torch-2.6.0+cu124:\n",
            "      Successfully uninstalled torch-2.6.0+cu124\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 1.13.1 which is incompatible.\n",
            "torchdata 0.11.0 requires torch>=2, but you have torch 1.13.1 which is incompatible.\n",
            "torchvision 0.21.0+cu124 requires torch==2.6.0, but you have torch 1.13.1 which is incompatible.\n",
            "accelerate 1.7.0 requires torch>=2.0.0, but you have torch 1.13.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 rtdl-0.0.13 torch-1.13.1\n",
            "Collecting git+https://github.com/yura52/rtdl.git\n",
            "  Cloning https://github.com/yura52/rtdl.git to /tmp/pip-req-build-5ankd90x\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/yura52/rtdl.git /tmp/pip-req-build-5ankd90x\n",
            "  Resolved https://github.com/yura52/rtdl.git to commit bdc2fe52e0c28d0d15a3cf7d99ef94a452f23253\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch<3,>=1.8 in /usr/local/lib/python3.11/dist-packages (from rtdl==0.0.14.dev7) (1.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.8->rtdl==0.0.14.dev7) (4.14.0)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.8->rtdl==0.0.14.dev7) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.8->rtdl==0.0.14.dev7) (8.5.0.96)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.8->rtdl==0.0.14.dev7) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.8->rtdl==0.0.14.dev7) (11.7.99)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch<3,>=1.8->rtdl==0.0.14.dev7) (75.2.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch<3,>=1.8->rtdl==0.0.14.dev7) (0.45.1)\n",
            "Building wheels for collected packages: rtdl\n",
            "  Building wheel for rtdl (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rtdl: filename=rtdl-0.0.14.dev7-py3-none-any.whl size=27164 sha256=02928dec0630719a18b1700de9c9a19bb3d213149c6c49f9aa5563a09ea09e43\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-1x9p4s2y/wheels/7f/7a/5c/88c72feb54756a9ded3401a0829d8be66129d21faf4e01224a\n",
            "Successfully built rtdl\n",
            "Installing collected packages: rtdl\n",
            "  Attempting uninstall: rtdl\n",
            "    Found existing installation: rtdl 0.0.13\n",
            "    Uninstalling rtdl-0.0.13:\n",
            "      Successfully uninstalled rtdl-0.0.13\n",
            "Successfully installed rtdl-0.0.14.dev7\n"
          ]
        }
      ],
      "source": [
        "!pip install \"numpy<2.0\" --force-reinstall\n",
        "! pip install rtdl\n",
        "! pip install git+https://github.com/yura52/rtdl.git\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22ad9bab",
      "metadata": {
        "id": "22ad9bab"
      },
      "outputs": [],
      "source": [
        "import os; os.kill(os.getpid(), 9)  #  ëŸ°íƒ€ì„ ì¬ì‹œì‘"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "pL6mik0zBngW",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pL6mik0zBngW",
        "outputId": "3379377f-2b6b-4714-cc8d-820e6ccc4878"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-4.3.0-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.16.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (24.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.41)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna) (1.1.3)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.14.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.2)\n",
            "Downloading optuna-4.3.0-py3-none-any.whl (386 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m386.6/386.6 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.16.1-py3-none-any.whl (242 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m242.5/242.5 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: colorlog, alembic, optuna\n",
            "Successfully installed alembic-1.16.1 colorlog-6.9.0 optuna-4.3.0\n"
          ]
        }
      ],
      "source": [
        "pip install optuna"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ad85b84",
      "metadata": {
        "id": "7ad85b84"
      },
      "source": [
        "### ë¼ì´ë¸ŒëŸ¬ë¦¬ í˜¸ì¶œ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "db85abc1",
      "metadata": {
        "id": "db85abc1"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# í•œê¸€ í°íŠ¸ ì„¤ì • (Windows ê¸°ë³¸ í°íŠ¸)\n",
        "plt.rcParams['font.family'] = 'Malgun Gothic'\n",
        "plt.rcParams['axes.unicode_minus'] = False  # ë§ˆì´ë„ˆìŠ¤ ê¹¨ì§ ë°©ì§€\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc2355ef",
      "metadata": {
        "id": "cc2355ef"
      },
      "source": [
        "### ì¸ì½”ë”©"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05638da7",
      "metadata": {
        "id": "05638da7"
      },
      "outputs": [],
      "source": [
        "df=pd.read_csv('../real_data/ê³µì‹œì§€ê°€_ì „ì²˜ë¦¬.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44c42384",
      "metadata": {
        "id": "44c42384"
      },
      "source": [
        "| êµ¬ë¶„   | ëª¨ë¸ëª…                               | ì„¤ëª…                                                  |\n",
        "| ---- | --------------------------------- | --------------------------------------------------- |\n",
        "|  1 | **CatBoost Regressor**            | ë²”ì£¼í˜• ë°ì´í„°ì™€ ì›-í•« ì¸ì½”ë”© ì—†ì´ë„ ê°•ë ¥í•œ ì„±ëŠ¥ (but ì´ë¯¸ ì›-í•«ì´ë©´ ê·¸ëƒ¥ ì‚¬ìš© ê°€ëŠ¥) |\n",
        "|  2 | **LightGBM Regressor**            | ì†ë„ ë¹ ë¥´ê³  ëŒ€ê·œëª¨ ë°ì´í„°ì— ê°•í•¨, ë¶€ìŠ¤íŒ… ê¸°ë°˜                          |\n",
        "|  3 | **XGBoost Regressor**             | ì•ˆì •ì ì´ê³  ë„ë¦¬ ì‚¬ìš©ë˜ëŠ” ë¶€ìŠ¤íŒ… ëª¨ë¸                                |\n",
        "|  4 | **HistGradientBoostingRegressor** | sklearn ìµœì‹  íŠ¸ë¦¬ ê¸°ë°˜ ëª¨ë¸. ë¹ ë¥´ê³  íŠœë‹ì´ ì‰¬ì›€                     |\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28922bd1",
      "metadata": {
        "id": "28922bd1"
      },
      "outputs": [],
      "source": [
        "# 1. X, y ë¶„ë¦¬\n",
        "X = df.drop(columns='ë‚´ë…„_ê³µì‹œì§€ê°€')\n",
        "y = df['ë‚´ë…„_ê³µì‹œì§€ê°€']\n",
        "\n",
        "# 1ì°¨ ë¶„í• : train_temp / test\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 2ì°¨ ë¶„í• : train / valid\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X_temp, y_temp, test_size=0.2, random_state=42)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "670c7f7e",
      "metadata": {
        "id": "670c7f7e"
      },
      "outputs": [],
      "source": [
        "# 1. ì—°ì†í˜• ë³€ìˆ˜ 'ë©´ì ' ì •ê·œí™” + ë³€í™˜\n",
        "scaler_area = MinMaxScaler()\n",
        "area_train_scaled = scaler_area.fit_transform(X_train[['ë©´ì ']])\n",
        "area_valid_scaled=scaler_area.transform(X_valid[['ë©´ì ']])\n",
        "area_test_scaled = scaler_area.transform(X_test[['ë©´ì ']])\n",
        "\n",
        "# 2. ë²”ì£¼í˜• ë³€ìˆ˜ ëª©ë¡ ì¶”ì¶œ (ë©´ì  ì œì™¸)\n",
        "categorical_cols = X_train.drop(columns=['ë©´ì ']).columns.tolist()\n",
        "\n",
        "# 3. OneHotEncoder í•™ìŠµ + ë³€í™˜\n",
        "encoder = OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore')\n",
        "X_train_cat_encoded = encoder.fit_transform(X_train[categorical_cols])\n",
        "X_valid_cat_encoded = encoder.transform(X_valid[categorical_cols])\n",
        "X_test_cat_encoded = encoder.transform(X_test[categorical_cols])\n",
        "\n",
        "# 4. ì¸ì½”ë”©ëœ íŠ¹ì„± ì´ë¦„\n",
        "encoded_feature_names = encoder.get_feature_names_out(categorical_cols)\n",
        "\n",
        "# 5. ìµœì¢… X_train,X_valid, X_test êµ¬ì„± -> ë©´ì ê³¼ ì›-í•« ì²˜ë¦¬ëœ ë³€ìˆ˜ë“¤ ê²°í•©\n",
        "X_train = pd.DataFrame(\n",
        "    np.concatenate([area_train_scaled, X_train_cat_encoded], axis=1),\n",
        "    columns=['ë©´ì '] + encoded_feature_names.tolist()\n",
        ")\n",
        "\n",
        "X_valid = pd.DataFrame(\n",
        "    np.concatenate([area_valid_scaled, X_valid_cat_encoded], axis=1),\n",
        "    columns=['ë©´ì '] + encoded_feature_names.tolist()\n",
        ")\n",
        "\n",
        "X_test = pd.DataFrame(\n",
        "    np.concatenate([area_test_scaled, X_test_cat_encoded], axis=1),\n",
        "    columns=['ë©´ì '] + encoded_feature_names.tolist()\n",
        ")\n",
        "\n",
        "# 6. ê³µì‹œì§€ê°€ ì •ê·œí™”\n",
        "scaler_target = MinMaxScaler()\n",
        "y_train = scaler_target.fit_transform(y_train.values.reshape(-1, 1))\n",
        "y_valid = scaler_target.transform(y_valid.values.reshape(-1, 1))\n",
        "y_test = scaler_target.transform(y_test.values.reshape(-1, 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b1ad939",
      "metadata": {
        "id": "8b1ad939"
      },
      "outputs": [],
      "source": [
        "# ë³€ìˆ˜ëª…ì— ì¡´ì¬í•˜ëŠ” , ë„ì–´ì“°ê¸° '_'ë¡œ ëŒ€ì²´\n",
        "\n",
        "X_train.columns = X_train.columns.str.replace(',', '_')\n",
        "X_train.columns = X_train.columns.str.replace(' ', '_')\n",
        "X_valid.columns = X_valid.columns.str.replace(',', '_')\n",
        "X_valid.columns = X_valid.columns.str.replace(' ', '_')\n",
        "X_test.columns = X_test.columns.str.replace(',', '_')\n",
        "X_test.columns = X_test.columns.str.replace(' ', '_')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9896b31",
      "metadata": {
        "id": "f9896b31"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "# ë¶„í•  ì €ì¥\n",
        "with open(\"./split_data/split_data_3way.pkl\", \"wb\") as f:\n",
        "    pickle.dump((X_train, X_valid, X_test, y_train, y_valid, y_test), f)\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c708c10",
      "metadata": {
        "id": "2c708c10"
      },
      "source": [
        "### ft-transformer ì¸ì½”ë”©"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee2983fa",
      "metadata": {
        "id": "ee2983fa"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset\n",
        "\n",
        "# 1. X, y ë¶„ë¦¬\n",
        "X = df.drop(columns='ë‚´ë…„_ê³µì‹œì§€ê°€')\n",
        "y = df['ë‚´ë…„_ê³µì‹œì§€ê°€']\n",
        "\n",
        "# 2. 3-way split ->random_state ì§€ì •ìœ¼ë¡œ ë™ì¼í•œ ë¶„ë¥˜\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X_temp, y_temp, test_size=0.2, random_state=42)\n",
        "\n",
        "# 3. ë©´ì  MinMax ìŠ¤ì¼€ì¼ë§\n",
        "scaler_area = MinMaxScaler()\n",
        "X_train['ë©´ì '] = scaler_area.fit_transform(X_train[['ë©´ì ']])\n",
        "X_valid['ë©´ì '] = scaler_area.transform(X_valid[['ë©´ì ']])\n",
        "X_test['ë©´ì '] = scaler_area.transform(X_test[['ë©´ì ']])\n",
        "\n",
        "# 4. íƒ€ê¹ƒ MinMax ìŠ¤ì¼€ì¼ë§\n",
        "scaler_target = MinMaxScaler()\n",
        "y_train = scaler_target.fit_transform(y_train.values.reshape(-1, 1))\n",
        "y_valid = scaler_target.transform(y_valid.values.reshape(-1, 1))\n",
        "y_test = scaler_target.transform(y_test.values.reshape(-1, 1))\n",
        "\n",
        "# 5. ëª…ëª©í˜• ë³€ìˆ˜ ë¼ë²¨ ì¸ì½”ë”© ->ft-transformer ëª¨ë¸ì€ ì›-í•«ì´ ì•„ë‹Œ,\n",
        "# ë¼ë²¨ ì¸ì½”ë”©ì´ ë” ì í•©.\n",
        "categorical_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "label_encoders = {}\n",
        "\n",
        "for col in categorical_cols:\n",
        "    le = LabelEncoder()\n",
        "    X_train[col] = le.fit_transform(X_train[col])\n",
        "    X_valid[col] = le.transform(X_valid[col])\n",
        "    X_test[col] = le.transform(X_test[col])\n",
        "    label_encoders[col] = le\n",
        "\n",
        "# ë³€ìˆ˜ëª…ì— ì¡´ì¬í•˜ëŠ” , ë„ì–´ì“°ê¸° ì œê±°\n",
        "\n",
        "X_train.columns = X_train.columns.str.replace(',', '_')\n",
        "X_train.columns = X_train.columns.str.replace(' ', '_')\n",
        "X_valid.columns = X_valid.columns.str.replace(',', '_')\n",
        "X_valid.columns = X_valid.columns.str.replace(' ', '_')\n",
        "X_test.columns = X_test.columns.str.replace(',', '_')\n",
        "X_test.columns = X_test.columns.str.replace(' ', '_')\n",
        "\n",
        "# 6. ë„˜íŒŒì´ë¡œ ë³€í™˜\n",
        "X_train_np = X_train.values.astype(np.float32)\n",
        "X_valid_np = X_valid.values.astype(np.float32)\n",
        "X_test_np = X_test.values.astype(np.float32)\n",
        "\n",
        "y_train_np = y_train.astype(np.float32)\n",
        "y_valid_np = y_valid.astype(np.float32)\n",
        "y_test_np = y_test.astype(np.float32)\n",
        "\n",
        "# 7. í…ì„œë¡œ ë³€í™˜ ft-transformerì€ í…ì„œí˜•íƒœë¡œ ì‚¬ìš©ë¨.\n",
        "X_train_tensor = torch.tensor(X_train_np)\n",
        "X_valid_tensor = torch.tensor(X_valid_np)\n",
        "X_test_tensor = torch.tensor(X_test_np)\n",
        "\n",
        "y_train_tensor = torch.tensor(y_train_np)\n",
        "y_valid_tensor = torch.tensor(y_valid_np)\n",
        "y_test_tensor = torch.tensor(y_test_np)\n",
        "\n",
        "# 8. Datasetìœ¼ë¡œ ìƒì„±\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "valid_dataset = TensorDataset(X_valid_tensor, y_valid_tensor)\n",
        "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c09221a2",
      "metadata": {
        "id": "c09221a2"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "with open(\"./split_data/split_data_ft_transformer.pkl\", \"wb\") as f:\n",
        "    pickle.dump((X_train_tensor, X_valid_tensor, X_test_tensor,\n",
        "                 y_train_tensor, y_valid_tensor, y_test_tensor), f)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "06a7f89e",
      "metadata": {
        "id": "06a7f89e"
      },
      "source": [
        "### ë¨¸ì‹ ëŸ¬ë‹ í•™ìŠµ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89bd9a6e",
      "metadata": {
        "id": "89bd9a6e"
      },
      "outputs": [],
      "source": [
        "# ì‹¤í—˜ ì¬í˜„\n",
        "with open(\"./split_data/split_data_3way.pkl\", \"rb\") as f:\n",
        "    X_train, X_valid, X_test, y_train, y_valid, y_test = pickle.load(f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0bb73ed1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0bb73ed1",
        "outputId": "417da39a-c385-464b-f913-46e31a54ab85"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ğŸ” [1/5] Random Search for CatBoost...\n",
            "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
            "âœ… CatBoost í•™ìŠµ ì™„ë£Œ\n",
            "ğŸ“Œ ìµœì  íŒŒë¼ë¯¸í„°: {'learning_rate': 0.05, 'l2_leaf_reg': 5, 'iterations': 400, 'depth': 8}\n",
            "ğŸ“Š í…ŒìŠ¤íŠ¸ RMSE: 0.009246\n",
            "ğŸ“ˆ í…ŒìŠ¤íŠ¸ RÂ²: 0.6731\n",
            "\n",
            "ğŸ” [2/5] Random Search for LightGBM...\n",
            "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016287 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 431\n",
            "[LightGBM] [Info] Number of data points in the train set: 298092, number of used features: 89\n",
            "[LightGBM] [Info] Start training from score 0.005130\n",
            "âœ… LightGBM í•™ìŠµ ì™„ë£Œ\n",
            "ğŸ“Œ ìµœì  íŒŒë¼ë¯¸í„°: {'num_leaves': 15, 'n_estimators': 500, 'min_child_samples': 20, 'max_depth': -1, 'learning_rate': 0.05}\n",
            "ğŸ“Š í…ŒìŠ¤íŠ¸ RMSE: 0.009272\n",
            "ğŸ“ˆ í…ŒìŠ¤íŠ¸ RÂ²: 0.6713\n",
            "\n",
            "ğŸ” [3/5] Random Search for XGBoost...\n",
            "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
            "âœ… XGBoost í•™ìŠµ ì™„ë£Œ\n",
            "ğŸ“Œ ìµœì  íŒŒë¼ë¯¸í„°: {'subsample': 1.0, 'n_estimators': 100, 'max_depth': 7, 'learning_rate': 0.2}\n",
            "ğŸ“Š í…ŒìŠ¤íŠ¸ RMSE: 0.009233\n",
            "ğŸ“ˆ í…ŒìŠ¤íŠ¸ RÂ²: 0.6741\n",
            "\n",
            "ğŸ” [4/5] Random Search for HistGBR...\n",
            "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
            "âœ… HistGBR í•™ìŠµ ì™„ë£Œ\n",
            "ğŸ“Œ ìµœì  íŒŒë¼ë¯¸í„°: {'min_samples_leaf': 50, 'max_iter': 500, 'max_depth': 10, 'learning_rate': 0.1, 'l2_regularization': 0.1}\n",
            "ğŸ“Š í…ŒìŠ¤íŠ¸ RMSE: 0.009310\n",
            "ğŸ“ˆ í…ŒìŠ¤íŠ¸ RÂ²: 0.6686\n",
            "\n",
            "ğŸ” [5/5] Random Search for ElasticNet...\n",
            "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
            "âœ… ElasticNet í•™ìŠµ ì™„ë£Œ\n",
            "ğŸ“Œ ìµœì  íŒŒë¼ë¯¸í„°: {'tol': 0.01, 'l1_ratio': 0.9, 'alpha': 10.0}\n",
            "ğŸ“Š í…ŒìŠ¤íŠ¸ RMSE: 0.016172\n",
            "ğŸ“ˆ í…ŒìŠ¤íŠ¸ RÂ²: -0.0000\n",
            "\n",
            "âœ… ìµœì¢… ê²°ê³¼ ìš”ì•½:\n",
            "ğŸ“Œ CatBoost | RMSE: 0.009246 | RÂ²: 0.6731 | Best Params: {'learning_rate': 0.05, 'l2_leaf_reg': 5, 'iterations': 400, 'depth': 8}\n",
            "ğŸ“Œ LightGBM | RMSE: 0.009272 | RÂ²: 0.6713 | Best Params: {'num_leaves': 15, 'n_estimators': 500, 'min_child_samples': 20, 'max_depth': -1, 'learning_rate': 0.05}\n",
            "ğŸ“Œ XGBoost | RMSE: 0.009233 | RÂ²: 0.6741 | Best Params: {'subsample': 1.0, 'n_estimators': 100, 'max_depth': 7, 'learning_rate': 0.2}\n",
            "ğŸ“Œ HistGBR | RMSE: 0.009310 | RÂ²: 0.6686 | Best Params: {'min_samples_leaf': 50, 'max_iter': 500, 'max_depth': 10, 'learning_rate': 0.1, 'l2_regularization': 0.1}\n",
            "ğŸ“Œ ElasticNet | RMSE: 0.016172 | RÂ²: -0.0000 | Best Params: {'tol': 0.01, 'l1_ratio': 0.9, 'alpha': 10.0}\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.ensemble import HistGradientBoostingRegressor, ExtraTreesRegressor\n",
        "from catboost import CatBoostRegressor\n",
        "from lightgbm import LGBMRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.linear_model import ElasticNet\n",
        "from sklearn.svm import SVR # ì‚¬ìš©X\n",
        "from sklearn.linear_model import Ridge # ì‚¬ìš©X\n",
        "import warnings\n",
        "import numpy as np\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "models = {\n",
        "    \"CatBoost\": (\n",
        "        CatBoostRegressor(verbose=0, random_state=42),\n",
        "        {\n",
        "            \"depth\": [4, 6, 8, 10],\n",
        "            \"learning_rate\": [0.01, 0.05, 0.1],\n",
        "            \"iterations\": [200, 400, 600], # ë°˜ë³µ íšŸìˆ˜\n",
        "            \"l2_leaf_reg\": [3, 5, 7] # ì •ê·œí™” ê³„ìˆ˜\n",
        "        }\n",
        "    ),\n",
        "    \"LightGBM\": (\n",
        "        LGBMRegressor( random_state=42),\n",
        "        {\n",
        "            \"num_leaves\": [15, 31, 63],\n",
        "            \"max_depth\": [-1, 5, 10, 20],\n",
        "            \"learning_rate\": [0.01, 0.05, 0.1],\n",
        "            \"n_estimators\": [100, 300, 500], # íŠ¸ë¦¬ê°œìˆ˜\n",
        "            \"min_child_samples\": [5, 10, 20] # ì ë‹¹ ìµœì†Œ ìƒ˜í”Œ ê°œìˆ˜\n",
        "        }\n",
        "    ),\n",
        "    \"XGBoost\": (\n",
        "        XGBRegressor(random_state=42, verbosity=0),\n",
        "        {\n",
        "            \"max_depth\": [3, 5, 7, 10],\n",
        "            \"learning_rate\": [0.01, 0.1, 0.2],\n",
        "            \"n_estimators\": [100, 300],\n",
        "            \"subsample\": [0.6, 0.8, 1.0] # ì‚¬ìš©í•  ìƒ˜í”Œ ë¹„ìœ¨\n",
        "        }\n",
        "    ),\n",
        "    \"HistGBR\": (\n",
        "        HistGradientBoostingRegressor(random_state=42),\n",
        "        {\n",
        "            \"max_iter\": [100, 300, 500],\n",
        "            \"learning_rate\": [0.01, 0.05, 0.1],\n",
        "            \"max_depth\": [None, 10, 20, 30],\n",
        "            \"min_samples_leaf\": [20, 50, 100],\n",
        "            \"l2_regularization\": [0.0, 0.1, 1.0]\n",
        "        }\n",
        "    ),\n",
        "    \"ElasticNet\": (\n",
        "    ElasticNet(random_state=42, max_iter=10000),\n",
        "    {\n",
        "        \"alpha\": [0.01, 0.1, 1.0, 10.0], # ì „ì²´ ê·œì œ ê°•ë„\n",
        "        \"l1_ratio\": [0.1, 0.5, 0.9],  # l1, l2ë¹„ìœ¨\n",
        "        \"tol\": [1e-4, 1e-3, 1e-2]  # ìˆ˜ë ´ í—ˆìš© ì˜¤ì°¨\n",
        "    }\n",
        ")\n",
        "}\n",
        "\n",
        "results = {}\n",
        "total_models = len(models)\n",
        "\n",
        "for i, (name, (model, param_grid)) in enumerate(models.items()):\n",
        "    print(f\"\\nğŸ” [{i+1}/{total_models}] Random Search for {name}...\")\n",
        "\n",
        "    search = RandomizedSearchCV(\n",
        "        estimator=model,\n",
        "        param_distributions=param_grid,\n",
        "        n_iter=10, # ì¡°í•© ìˆ˜\n",
        "        scoring=\"neg_mean_squared_error\", # ìµœì†Œ mseê°’\n",
        "        cv=3,    # êµì°¨ ê²€ì¦ íšŸìˆ˜\n",
        "        n_jobs=-1, # cpu ë³‘ë ¬ì²˜ë¦¬\n",
        "        verbose=1,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    search.fit(X_train, y_train.ravel())\n",
        "\n",
        "    best_model = search.best_estimator_\n",
        "    y_pred = best_model.predict(X_test)\n",
        "\n",
        "    # í‰ê°€ ì§€í‘œ ê³„ì‚°\n",
        "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "    results[name] = {\n",
        "        \"best_params\": search.best_params_,\n",
        "        \"test_rmse\": rmse,\n",
        "        \"test_r2\": r2\n",
        "    }\n",
        "\n",
        "    # ê°œë³„ ê²°ê³¼ ì¦‰ì‹œ ì¶œë ¥\n",
        "    print(f\"âœ… {name} í•™ìŠµ ì™„ë£Œ\")\n",
        "    print(f\"ğŸ“Œ ìµœì  íŒŒë¼ë¯¸í„°: {search.best_params_}\")\n",
        "    print(f\"ğŸ“Š í…ŒìŠ¤íŠ¸ RMSE: {rmse:.6f}\")\n",
        "    print(f\"ğŸ“ˆ í…ŒìŠ¤íŠ¸ RÂ²: {r2:.4f}\")\n",
        "\n",
        "# ì „ì²´ ê²°ê³¼ ìš”ì•½\n",
        "print(\"\\nâœ… ìµœì¢… ê²°ê³¼ ìš”ì•½:\")\n",
        "for model_name, res in results.items():\n",
        "    print(f\"ğŸ“Œ {model_name} | RMSE: {res['test_rmse']:.6f} | RÂ²: {res['test_r2']:.4f} | Best Params: {res['best_params']}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "76089ffa",
      "metadata": {
        "id": "76089ffa"
      },
      "source": [
        "ElasticNET ì„±ëŠ¥ì´ ë„ˆë¬´ ì•ˆì¢‹ì•„, RandomForest Regressor ì¶”ê°€"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09b8c1f8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "09b8c1f8",
        "outputId": "de56ea84-3bd0-4054-a9cc-e8e64e254734"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ” Random Forest í•˜ì´í¼íŒŒë¼ë¯¸í„° íƒìƒ‰ ì¤‘...\n",
            "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
            "âœ… RandomForest í•™ìŠµ ì™„ë£Œ\n",
            "ğŸ“Œ ìµœì  íŒŒë¼ë¯¸í„°: {'n_estimators': 300, 'min_samples_split': 6, 'min_samples_leaf': 2, 'max_depth': 10}\n",
            "ğŸ“Š í…ŒìŠ¤íŠ¸ RMSE: 0.009356\n",
            "ğŸ“ˆ í…ŒìŠ¤íŠ¸ RÂ²: 0.6653\n",
            "â± ì‹¤í–‰ ì‹œê°„: 14703.43ì´ˆ\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import time\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import warnings\n",
        "from tqdm import tqdm\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# â± ì‹œê°„ ì¸¡ì • ì‹œì‘\n",
        "start = time.time()\n",
        "\n",
        "# 1. ëª¨ë¸ ë° íŒŒë¼ë¯¸í„° í›„ë³´ ì •ì˜\n",
        "rf_model = RandomForestRegressor(random_state=42)\n",
        "rf_param_grid = {\n",
        "    \"n_estimators\": [100, 200, 300],\n",
        "    \"max_depth\": [10, 15, 20],\n",
        "    \"min_samples_split\": [2, 4, 6],\n",
        "    \"min_samples_leaf\": [1, 2]\n",
        "}\n",
        "\n",
        "# 2. ëœë¤ ì„œì¹˜\n",
        "search = RandomizedSearchCV(\n",
        "    estimator=rf_model,\n",
        "    param_distributions=rf_param_grid,\n",
        "    n_iter=10,\n",
        "    scoring=\"neg_mean_squared_error\",\n",
        "    cv=3,\n",
        "    n_jobs=-1,\n",
        "    verbose=1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(\"ğŸ” Random Forest í•˜ì´í¼íŒŒë¼ë¯¸í„° íƒìƒ‰ ì¤‘...\")\n",
        "search.fit(X_train, y_train.ravel())\n",
        "\n",
        "# 3. ì˜ˆì¸¡ ë° í‰ê°€\n",
        "best_rf = search.best_estimator_\n",
        "y_pred = best_rf.predict(X_test)\n",
        "\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "# 4. ê²°ê³¼ ì¶œë ¥\n",
        "print(f\"âœ… RandomForest í•™ìŠµ ì™„ë£Œ\")\n",
        "print(f\"ğŸ“Œ ìµœì  íŒŒë¼ë¯¸í„°: {search.best_params_}\")\n",
        "print(f\"ğŸ“Š í…ŒìŠ¤íŠ¸ RMSE: {rmse:.6f}\")\n",
        "print(f\"ğŸ“ˆ í…ŒìŠ¤íŠ¸ RÂ²: {r2:.4f}\")\n",
        "\n",
        "# â± ì‹œê°„ ì¸¡ì • ì¢…ë£Œ\n",
        "print(f\"â± ì‹¤í–‰ ì‹œê°„: {(time.time() - start):.2f}ì´ˆ\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d6d7b8c4",
      "metadata": {
        "id": "d6d7b8c4"
      },
      "source": [
        "catboost í™•ì¥ íƒìƒ‰"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6381b0a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a6381b0a",
        "outputId": "2d9b06b5-b037-44b9-b1e7-ac254a3d1794"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n",
            "0:\tlearn: 0.0149432\ttotal: 7.22ms\tremaining: 7.21s\n",
            "5:\tlearn: 0.0116791\ttotal: 39.5ms\tremaining: 6.54s\n",
            "10:\tlearn: 0.0101915\ttotal: 70.6ms\tremaining: 6.34s\n",
            "15:\tlearn: 0.0095158\ttotal: 103ms\tremaining: 6.33s\n",
            "20:\tlearn: 0.0092019\ttotal: 134ms\tremaining: 6.24s\n",
            "25:\tlearn: 0.0090190\ttotal: 165ms\tremaining: 6.17s\n",
            "30:\tlearn: 0.0089109\ttotal: 193ms\tremaining: 6.02s\n",
            "35:\tlearn: 0.0088397\ttotal: 219ms\tremaining: 5.86s\n",
            "40:\tlearn: 0.0087867\ttotal: 242ms\tremaining: 5.67s\n",
            "45:\tlearn: 0.0087358\ttotal: 271ms\tremaining: 5.61s\n",
            "50:\tlearn: 0.0086876\ttotal: 295ms\tremaining: 5.48s\n",
            "55:\tlearn: 0.0086548\ttotal: 318ms\tremaining: 5.37s\n",
            "60:\tlearn: 0.0086318\ttotal: 342ms\tremaining: 5.26s\n",
            "65:\tlearn: 0.0085985\ttotal: 365ms\tremaining: 5.17s\n",
            "70:\tlearn: 0.0085688\ttotal: 388ms\tremaining: 5.08s\n",
            "75:\tlearn: 0.0085453\ttotal: 407ms\tremaining: 4.95s\n",
            "80:\tlearn: 0.0085131\ttotal: 429ms\tremaining: 4.86s\n",
            "85:\tlearn: 0.0084926\ttotal: 448ms\tremaining: 4.76s\n",
            "90:\tlearn: 0.0084643\ttotal: 467ms\tremaining: 4.67s\n",
            "95:\tlearn: 0.0084459\ttotal: 488ms\tremaining: 4.59s\n",
            "100:\tlearn: 0.0084277\ttotal: 536ms\tremaining: 4.77s\n",
            "105:\tlearn: 0.0084051\ttotal: 557ms\tremaining: 4.7s\n",
            "110:\tlearn: 0.0083936\ttotal: 578ms\tremaining: 4.63s\n",
            "115:\tlearn: 0.0083749\ttotal: 598ms\tremaining: 4.55s\n",
            "120:\tlearn: 0.0083572\ttotal: 618ms\tremaining: 4.49s\n",
            "125:\tlearn: 0.0083380\ttotal: 640ms\tremaining: 4.44s\n",
            "130:\tlearn: 0.0083184\ttotal: 660ms\tremaining: 4.38s\n",
            "135:\tlearn: 0.0083035\ttotal: 682ms\tremaining: 4.33s\n",
            "140:\tlearn: 0.0082907\ttotal: 702ms\tremaining: 4.28s\n",
            "145:\tlearn: 0.0082771\ttotal: 722ms\tremaining: 4.22s\n",
            "150:\tlearn: 0.0082659\ttotal: 742ms\tremaining: 4.17s\n",
            "155:\tlearn: 0.0082529\ttotal: 763ms\tremaining: 4.13s\n",
            "160:\tlearn: 0.0082410\ttotal: 782ms\tremaining: 4.08s\n",
            "165:\tlearn: 0.0082268\ttotal: 803ms\tremaining: 4.03s\n",
            "170:\tlearn: 0.0082142\ttotal: 823ms\tremaining: 3.99s\n",
            "175:\tlearn: 0.0082064\ttotal: 845ms\tremaining: 3.95s\n",
            "180:\tlearn: 0.0081959\ttotal: 864ms\tremaining: 3.91s\n",
            "185:\tlearn: 0.0081780\ttotal: 884ms\tremaining: 3.87s\n",
            "190:\tlearn: 0.0081645\ttotal: 904ms\tremaining: 3.83s\n",
            "195:\tlearn: 0.0081520\ttotal: 924ms\tremaining: 3.79s\n",
            "200:\tlearn: 0.0081436\ttotal: 945ms\tremaining: 3.75s\n",
            "205:\tlearn: 0.0081334\ttotal: 965ms\tremaining: 3.72s\n",
            "210:\tlearn: 0.0081240\ttotal: 986ms\tremaining: 3.69s\n",
            "215:\tlearn: 0.0081164\ttotal: 1s\tremaining: 3.65s\n",
            "220:\tlearn: 0.0081070\ttotal: 1.02s\tremaining: 3.61s\n",
            "225:\tlearn: 0.0080998\ttotal: 1.05s\tremaining: 3.59s\n",
            "230:\tlearn: 0.0080907\ttotal: 1.07s\tremaining: 3.56s\n",
            "235:\tlearn: 0.0080786\ttotal: 1.09s\tremaining: 3.52s\n",
            "240:\tlearn: 0.0080690\ttotal: 1.11s\tremaining: 3.49s\n",
            "245:\tlearn: 0.0080600\ttotal: 1.13s\tremaining: 3.46s\n",
            "250:\tlearn: 0.0080527\ttotal: 1.15s\tremaining: 3.42s\n",
            "255:\tlearn: 0.0080446\ttotal: 1.17s\tremaining: 3.39s\n",
            "260:\tlearn: 0.0080351\ttotal: 1.19s\tremaining: 3.36s\n",
            "265:\tlearn: 0.0080314\ttotal: 1.21s\tremaining: 3.33s\n",
            "270:\tlearn: 0.0080252\ttotal: 1.23s\tremaining: 3.3s\n",
            "275:\tlearn: 0.0080153\ttotal: 1.25s\tremaining: 3.27s\n",
            "280:\tlearn: 0.0080083\ttotal: 1.27s\tremaining: 3.26s\n",
            "285:\tlearn: 0.0080038\ttotal: 1.29s\tremaining: 3.23s\n",
            "290:\tlearn: 0.0079984\ttotal: 1.31s\tremaining: 3.19s\n",
            "295:\tlearn: 0.0079896\ttotal: 1.33s\tremaining: 3.17s\n",
            "300:\tlearn: 0.0079832\ttotal: 1.35s\tremaining: 3.14s\n",
            "305:\tlearn: 0.0079714\ttotal: 1.37s\tremaining: 3.11s\n",
            "310:\tlearn: 0.0079646\ttotal: 1.39s\tremaining: 3.08s\n",
            "315:\tlearn: 0.0079577\ttotal: 1.41s\tremaining: 3.06s\n",
            "320:\tlearn: 0.0079518\ttotal: 1.43s\tremaining: 3.03s\n",
            "325:\tlearn: 0.0079472\ttotal: 1.45s\tremaining: 3.01s\n",
            "330:\tlearn: 0.0079402\ttotal: 1.47s\tremaining: 2.98s\n",
            "335:\tlearn: 0.0079306\ttotal: 1.49s\tremaining: 2.95s\n",
            "340:\tlearn: 0.0079263\ttotal: 1.54s\tremaining: 2.97s\n",
            "345:\tlearn: 0.0079183\ttotal: 1.56s\tremaining: 2.95s\n",
            "350:\tlearn: 0.0079152\ttotal: 1.58s\tremaining: 2.92s\n",
            "355:\tlearn: 0.0079113\ttotal: 1.6s\tremaining: 2.89s\n",
            "360:\tlearn: 0.0079072\ttotal: 1.62s\tremaining: 2.87s\n",
            "365:\tlearn: 0.0079033\ttotal: 1.64s\tremaining: 2.84s\n",
            "370:\tlearn: 0.0078996\ttotal: 1.66s\tremaining: 2.82s\n",
            "375:\tlearn: 0.0078927\ttotal: 1.68s\tremaining: 2.79s\n",
            "380:\tlearn: 0.0078869\ttotal: 1.7s\tremaining: 2.76s\n",
            "385:\tlearn: 0.0078836\ttotal: 1.72s\tremaining: 2.74s\n",
            "390:\tlearn: 0.0078801\ttotal: 1.74s\tremaining: 2.71s\n",
            "395:\tlearn: 0.0078696\ttotal: 1.76s\tremaining: 2.69s\n",
            "400:\tlearn: 0.0078653\ttotal: 1.78s\tremaining: 2.66s\n",
            "405:\tlearn: 0.0078605\ttotal: 1.8s\tremaining: 2.63s\n",
            "410:\tlearn: 0.0078550\ttotal: 1.82s\tremaining: 2.61s\n",
            "415:\tlearn: 0.0078513\ttotal: 1.84s\tremaining: 2.58s\n",
            "420:\tlearn: 0.0078455\ttotal: 1.86s\tremaining: 2.56s\n",
            "425:\tlearn: 0.0078367\ttotal: 1.88s\tremaining: 2.54s\n",
            "430:\tlearn: 0.0078307\ttotal: 1.9s\tremaining: 2.51s\n",
            "435:\tlearn: 0.0078251\ttotal: 1.92s\tremaining: 2.49s\n",
            "440:\tlearn: 0.0078216\ttotal: 1.94s\tremaining: 2.46s\n",
            "445:\tlearn: 0.0078167\ttotal: 1.96s\tremaining: 2.44s\n",
            "450:\tlearn: 0.0078075\ttotal: 1.98s\tremaining: 2.41s\n",
            "455:\tlearn: 0.0078044\ttotal: 2s\tremaining: 2.39s\n",
            "460:\tlearn: 0.0078008\ttotal: 2.02s\tremaining: 2.36s\n",
            "465:\tlearn: 0.0077983\ttotal: 2.04s\tremaining: 2.34s\n",
            "470:\tlearn: 0.0077920\ttotal: 2.06s\tremaining: 2.32s\n",
            "475:\tlearn: 0.0077886\ttotal: 2.08s\tremaining: 2.29s\n",
            "480:\tlearn: 0.0077860\ttotal: 2.1s\tremaining: 2.27s\n",
            "485:\tlearn: 0.0077832\ttotal: 2.12s\tremaining: 2.25s\n",
            "490:\tlearn: 0.0077788\ttotal: 2.14s\tremaining: 2.22s\n",
            "495:\tlearn: 0.0077721\ttotal: 2.16s\tremaining: 2.2s\n",
            "500:\tlearn: 0.0077684\ttotal: 2.18s\tremaining: 2.17s\n",
            "505:\tlearn: 0.0077645\ttotal: 2.2s\tremaining: 2.15s\n",
            "510:\tlearn: 0.0077613\ttotal: 2.22s\tremaining: 2.12s\n",
            "515:\tlearn: 0.0077558\ttotal: 2.25s\tremaining: 2.1s\n",
            "520:\tlearn: 0.0077522\ttotal: 2.26s\tremaining: 2.08s\n",
            "525:\tlearn: 0.0077480\ttotal: 2.29s\tremaining: 2.06s\n",
            "530:\tlearn: 0.0077420\ttotal: 2.31s\tremaining: 2.04s\n",
            "535:\tlearn: 0.0077368\ttotal: 2.33s\tremaining: 2.01s\n",
            "540:\tlearn: 0.0077320\ttotal: 2.35s\tremaining: 1.99s\n",
            "545:\tlearn: 0.0077259\ttotal: 2.37s\tremaining: 1.97s\n",
            "550:\tlearn: 0.0077230\ttotal: 2.39s\tremaining: 1.94s\n",
            "555:\tlearn: 0.0077161\ttotal: 2.4s\tremaining: 1.92s\n",
            "560:\tlearn: 0.0077116\ttotal: 2.42s\tremaining: 1.9s\n",
            "565:\tlearn: 0.0077020\ttotal: 2.44s\tremaining: 1.87s\n",
            "570:\tlearn: 0.0076977\ttotal: 2.46s\tremaining: 1.85s\n",
            "575:\tlearn: 0.0076899\ttotal: 2.48s\tremaining: 1.83s\n",
            "580:\tlearn: 0.0076875\ttotal: 2.51s\tremaining: 1.81s\n",
            "585:\tlearn: 0.0076836\ttotal: 2.53s\tremaining: 1.78s\n",
            "590:\tlearn: 0.0076806\ttotal: 2.57s\tremaining: 1.78s\n",
            "595:\tlearn: 0.0076770\ttotal: 2.59s\tremaining: 1.76s\n",
            "600:\tlearn: 0.0076745\ttotal: 2.61s\tremaining: 1.74s\n",
            "605:\tlearn: 0.0076722\ttotal: 2.63s\tremaining: 1.71s\n",
            "610:\tlearn: 0.0076690\ttotal: 2.66s\tremaining: 1.69s\n",
            "615:\tlearn: 0.0076637\ttotal: 2.68s\tremaining: 1.67s\n",
            "620:\tlearn: 0.0076604\ttotal: 2.7s\tremaining: 1.65s\n",
            "625:\tlearn: 0.0076529\ttotal: 2.72s\tremaining: 1.63s\n",
            "630:\tlearn: 0.0076510\ttotal: 2.74s\tremaining: 1.6s\n",
            "635:\tlearn: 0.0076493\ttotal: 2.76s\tremaining: 1.58s\n",
            "640:\tlearn: 0.0076449\ttotal: 2.78s\tremaining: 1.56s\n",
            "645:\tlearn: 0.0076413\ttotal: 2.8s\tremaining: 1.53s\n",
            "650:\tlearn: 0.0076388\ttotal: 2.82s\tremaining: 1.51s\n",
            "655:\tlearn: 0.0076347\ttotal: 2.84s\tremaining: 1.49s\n",
            "660:\tlearn: 0.0076300\ttotal: 2.86s\tremaining: 1.47s\n",
            "665:\tlearn: 0.0076270\ttotal: 2.88s\tremaining: 1.45s\n",
            "670:\tlearn: 0.0076229\ttotal: 2.9s\tremaining: 1.42s\n",
            "675:\tlearn: 0.0076207\ttotal: 2.92s\tremaining: 1.4s\n",
            "680:\tlearn: 0.0076156\ttotal: 2.94s\tremaining: 1.38s\n",
            "685:\tlearn: 0.0076099\ttotal: 2.96s\tremaining: 1.36s\n",
            "690:\tlearn: 0.0076058\ttotal: 2.98s\tremaining: 1.33s\n",
            "695:\tlearn: 0.0076001\ttotal: 3s\tremaining: 1.31s\n",
            "700:\tlearn: 0.0075924\ttotal: 3.02s\tremaining: 1.29s\n",
            "705:\tlearn: 0.0075870\ttotal: 3.05s\tremaining: 1.27s\n",
            "710:\tlearn: 0.0075832\ttotal: 3.06s\tremaining: 1.25s\n",
            "715:\tlearn: 0.0075813\ttotal: 3.09s\tremaining: 1.22s\n",
            "720:\tlearn: 0.0075790\ttotal: 3.11s\tremaining: 1.2s\n",
            "725:\tlearn: 0.0075767\ttotal: 3.13s\tremaining: 1.18s\n",
            "730:\tlearn: 0.0075730\ttotal: 3.15s\tremaining: 1.16s\n",
            "735:\tlearn: 0.0075695\ttotal: 3.17s\tremaining: 1.14s\n",
            "740:\tlearn: 0.0075674\ttotal: 3.19s\tremaining: 1.11s\n",
            "745:\tlearn: 0.0075627\ttotal: 3.21s\tremaining: 1.09s\n",
            "750:\tlearn: 0.0075597\ttotal: 3.23s\tremaining: 1.07s\n",
            "755:\tlearn: 0.0075575\ttotal: 3.25s\tremaining: 1.05s\n",
            "760:\tlearn: 0.0075540\ttotal: 3.27s\tremaining: 1.03s\n",
            "765:\tlearn: 0.0075514\ttotal: 3.29s\tremaining: 1s\n",
            "770:\tlearn: 0.0075433\ttotal: 3.31s\tremaining: 983ms\n",
            "775:\tlearn: 0.0075411\ttotal: 3.33s\tremaining: 962ms\n",
            "780:\tlearn: 0.0075374\ttotal: 3.35s\tremaining: 940ms\n",
            "785:\tlearn: 0.0075338\ttotal: 3.37s\tremaining: 918ms\n",
            "790:\tlearn: 0.0075315\ttotal: 3.39s\tremaining: 896ms\n",
            "795:\tlearn: 0.0075287\ttotal: 3.41s\tremaining: 874ms\n",
            "800:\tlearn: 0.0075249\ttotal: 3.43s\tremaining: 852ms\n",
            "805:\tlearn: 0.0075232\ttotal: 3.45s\tremaining: 830ms\n",
            "810:\tlearn: 0.0075193\ttotal: 3.47s\tremaining: 808ms\n",
            "815:\tlearn: 0.0075174\ttotal: 3.49s\tremaining: 787ms\n",
            "820:\tlearn: 0.0075159\ttotal: 3.51s\tremaining: 765ms\n",
            "825:\tlearn: 0.0075140\ttotal: 3.53s\tremaining: 743ms\n",
            "830:\tlearn: 0.0075109\ttotal: 3.56s\tremaining: 723ms\n",
            "835:\tlearn: 0.0075081\ttotal: 3.59s\tremaining: 705ms\n",
            "840:\tlearn: 0.0075045\ttotal: 3.61s\tremaining: 683ms\n",
            "845:\tlearn: 0.0075004\ttotal: 3.63s\tremaining: 661ms\n",
            "850:\tlearn: 0.0074974\ttotal: 3.65s\tremaining: 640ms\n",
            "855:\tlearn: 0.0074953\ttotal: 3.67s\tremaining: 618ms\n",
            "860:\tlearn: 0.0074911\ttotal: 3.69s\tremaining: 596ms\n",
            "865:\tlearn: 0.0074893\ttotal: 3.71s\tremaining: 574ms\n",
            "870:\tlearn: 0.0074841\ttotal: 3.73s\tremaining: 553ms\n",
            "875:\tlearn: 0.0074800\ttotal: 3.75s\tremaining: 531ms\n",
            "880:\tlearn: 0.0074782\ttotal: 3.77s\tremaining: 509ms\n",
            "885:\tlearn: 0.0074743\ttotal: 3.79s\tremaining: 488ms\n",
            "890:\tlearn: 0.0074715\ttotal: 3.81s\tremaining: 466ms\n",
            "895:\tlearn: 0.0074688\ttotal: 3.83s\tremaining: 445ms\n",
            "900:\tlearn: 0.0074669\ttotal: 3.85s\tremaining: 423ms\n",
            "905:\tlearn: 0.0074638\ttotal: 3.87s\tremaining: 401ms\n",
            "910:\tlearn: 0.0074586\ttotal: 3.89s\tremaining: 380ms\n",
            "915:\tlearn: 0.0074562\ttotal: 3.91s\tremaining: 359ms\n",
            "920:\tlearn: 0.0074528\ttotal: 3.93s\tremaining: 337ms\n",
            "925:\tlearn: 0.0074504\ttotal: 3.95s\tremaining: 316ms\n",
            "930:\tlearn: 0.0074476\ttotal: 3.97s\tremaining: 294ms\n",
            "935:\tlearn: 0.0074456\ttotal: 3.99s\tremaining: 273ms\n",
            "940:\tlearn: 0.0074434\ttotal: 4.01s\tremaining: 251ms\n",
            "945:\tlearn: 0.0074420\ttotal: 4.03s\tremaining: 230ms\n",
            "950:\tlearn: 0.0074383\ttotal: 4.05s\tremaining: 209ms\n",
            "955:\tlearn: 0.0074357\ttotal: 4.07s\tremaining: 187ms\n",
            "960:\tlearn: 0.0074339\ttotal: 4.09s\tremaining: 166ms\n",
            "965:\tlearn: 0.0074320\ttotal: 4.11s\tremaining: 144ms\n",
            "970:\tlearn: 0.0074295\ttotal: 4.12s\tremaining: 123ms\n",
            "975:\tlearn: 0.0074253\ttotal: 4.14s\tremaining: 102ms\n",
            "980:\tlearn: 0.0074227\ttotal: 4.16s\tremaining: 80.6ms\n",
            "985:\tlearn: 0.0074199\ttotal: 4.18s\tremaining: 59.4ms\n",
            "990:\tlearn: 0.0074179\ttotal: 4.21s\tremaining: 38.2ms\n",
            "995:\tlearn: 0.0074148\ttotal: 4.23s\tremaining: 17ms\n",
            "999:\tlearn: 0.0074132\ttotal: 4.25s\tremaining: 0us\n",
            "\n",
            "âœ… CatBoost ì •ë°€ íƒìƒ‰ ì™„ë£Œ\n",
            "ğŸ“Œ ìµœì  íŒŒë¼ë¯¸í„°: {'learning_rate': 0.1, 'l2_leaf_reg': 3, 'iterations': 1000, 'depth': 6, 'border_count': 128, 'bagging_temperature': 0.0}\n",
            "ğŸ“Š í…ŒìŠ¤íŠ¸ RMSE: 0.009174\n",
            "ğŸ“ˆ í…ŒìŠ¤íŠ¸ RÂ²: 0.6782\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from catboost import CatBoostRegressor\n",
        "import numpy as np\n",
        "\n",
        "# CatBoost í™•ì¥ëœ íƒìƒ‰ ë²”ìœ„\n",
        "catboost_params = {\n",
        "    \"depth\": [4, 6, 8, 10, 12],\n",
        "    \"learning_rate\": [0.01, 0.03, 0.05, 0.07, 0.1],\n",
        "    \"iterations\": [300, 500, 700, 1000], # íŠ¸ë¦¬ ê°œìˆ˜\n",
        "    \"l2_leaf_reg\": [1, 3, 5, 7, 9], # l2ì •ê·œí™” ê°•ë„\n",
        "    \"bagging_temperature\": [0.0, 0.5, 1.0, 1.5], # ëœë¤ì„± ì¡°ì ˆ íŒŒë¼ë¯¸í„°\n",
        "    \"border_count\": [32, 64, 128] # ì—°ì†í˜• ë³€ìˆ˜ë¥¼ ì´ì‚°í˜•ìœ¼ë¡œ ë‚˜ëˆŒ ë•Œ ê²½ê³„ ê°œìˆ˜\n",
        "}\n",
        "\n",
        "catboost = CatBoostRegressor(task_type='GPU', verbose=5, random_state=42)\n",
        "\n",
        "search = RandomizedSearchCV(\n",
        "    estimator=catboost,\n",
        "    param_distributions=catboost_params,\n",
        "    n_iter=30,  # ê¹Šê²Œ íƒìƒ‰\n",
        "    scoring=\"neg_mean_squared_error\",\n",
        "    cv=3,\n",
        "    n_jobs=-1,\n",
        "    verbose=1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "search.fit(X_train, y_train.ravel())\n",
        "\n",
        "# í‰ê°€\n",
        "best_model = search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f\"\\nâœ… CatBoost ì •ë°€ íƒìƒ‰ ì™„ë£Œ\")\n",
        "print(f\"ğŸ“Œ ìµœì  íŒŒë¼ë¯¸í„°: {search.best_params_}\")\n",
        "print(f\"ğŸ“Š í…ŒìŠ¤íŠ¸ RMSE: {rmse:.6f}\")\n",
        "print(f\"ğŸ“ˆ í…ŒìŠ¤íŠ¸ RÂ²: {r2:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc31c895",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cc31c895",
        "outputId": "59984a37-1769-4982-a69d-c155704572b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.049544 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 431\n",
            "[LightGBM] [Info] Number of data points in the train set: 298092, number of used features: 89\n",
            "[LightGBM] [Info] Start training from score 0.005130\n",
            "ğŸ“Š ì—­ì •ê·œí™” ê¸°ì¤€ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ:\n",
            "âœ… CatBoost   | RMSE: 1,518,386.06 | RÂ²: 0.6774\n",
            "âœ… LightGBM   | RMSE: 1,532,644.61 | RÂ²: 0.6713\n",
            "âœ… XGBoost    | RMSE: 1,526,182.97 | RÂ²: 0.6741\n",
            "âœ… HistGBR    | RMSE: 1,538,908.34 | RÂ²: 0.6686\n",
            "âœ… RandomForest | RMSE: 1,546,480.83 | RÂ²: 0.6653\n"
          ]
        }
      ],
      "source": [
        "from catboost import CatBoostRegressor\n",
        "from lightgbm import LGBMRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.ensemble import HistGradientBoostingRegressor\n",
        "from sklearn.linear_model import ElasticNet\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import numpy as np\n",
        "\n",
        "# 1. Min-Max ì •ê·œí™” ê¸°ì¤€ê°’\n",
        "y_min = 216.0\n",
        "y_max = 165300000.0\n",
        "\n",
        "# 2. ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„° ì ìš© ëª¨ë¸ ì •ì˜\n",
        "models = {\n",
        "    \"CatBoost\": CatBoostRegressor(\n",
        "        learning_rate=0.1,\n",
        "        l2_leaf_reg=3,\n",
        "        iterations=1000,\n",
        "        depth=6,\n",
        "        border_count=128,\n",
        "        verbose=0,\n",
        "        random_state=42\n",
        "    ),\n",
        "    \"LightGBM\": LGBMRegressor(\n",
        "        num_leaves=15,\n",
        "        n_estimators=500,\n",
        "        min_child_samples=20,\n",
        "        max_depth=-1,\n",
        "        learning_rate=0.05,\n",
        "        random_state=42\n",
        "    ),\n",
        "    \"XGBoost\": XGBRegressor(\n",
        "        subsample=1.0,\n",
        "        n_estimators=100,\n",
        "        max_depth=7,\n",
        "        learning_rate=0.2,\n",
        "        random_state=42,\n",
        "        verbosity=0\n",
        "    ),\n",
        "    \"HistGBR\": HistGradientBoostingRegressor(\n",
        "        min_samples_leaf=50,\n",
        "        max_iter=500,\n",
        "        max_depth=10,\n",
        "        learning_rate=0.1,\n",
        "        l2_regularization=0.1,\n",
        "        random_state=42\n",
        "    ),\n",
        "    \"RandomForest\": RandomForestRegressor(\n",
        "        n_estimators=300,\n",
        "        min_samples_split=6,\n",
        "        min_samples_leaf=2,\n",
        "        max_depth=10,\n",
        "        random_state=42\n",
        "    )\n",
        "}\n",
        "\n",
        "# 3. ê²°ê³¼ ì €ì¥ìš© ë”•ì…”ë„ˆë¦¬\n",
        "results = {}\n",
        "\n",
        "# 4. ëª¨ë¸ í•™ìŠµ â†’ ì˜ˆì¸¡ â†’ ì—­ì •ê·œí™” â†’ ì„±ëŠ¥ í‰ê°€\n",
        "for name, model in models.items():\n",
        "    model.fit(X_train, y_train)\n",
        "    preds = model.predict(X_test)\n",
        "\n",
        "    # ì—­ì •ê·œí™”\n",
        "    preds_inverse = preds * (y_max - y_min) + y_min\n",
        "    y_test_inverse = y_test * (y_max - y_min) + y_min\n",
        "\n",
        "    # ì„±ëŠ¥ í‰ê°€\n",
        "    mse = mean_squared_error(y_test_inverse, preds_inverse)\n",
        "    rmse = np.sqrt(mse)\n",
        "    r2 = r2_score(y_test_inverse, preds_inverse)\n",
        "\n",
        "    results[name] = {\n",
        "        \"RMSE\": round(rmse, 2),\n",
        "        \"RÂ²\": round(r2, 4)\n",
        "    }\n",
        "\n",
        "# 5. ê²°ê³¼ ì¶œë ¥\n",
        "print(\"ğŸ“Š ì—­ì •ê·œí™” ê¸°ì¤€ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ:\")\n",
        "for name, res in results.items():\n",
        "    print(f\"âœ… {name:<10} | RMSE: {res['RMSE']:,} | RÂ²: {res['RÂ²']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c98258c1",
      "metadata": {
        "id": "c98258c1"
      },
      "source": [
        "### tabnet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "375e8747",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "375e8747",
        "outputId": "aeb1bd23-6639-4b3c-a50d-aa5dbf695de1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-05-23 16:55:41,741] A new study created in memory with name: no-name-9acb0fcb-e231-481d-9ee4-bb92e68ac524\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Early stopping occurred at epoch 57 with best_epoch = 47 and best_val_0_rmse = 0.01043\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "[I 2025-05-23 17:17:41,598] Trial 0 finished with value: 0.010426763997343673 and parameters: {'n_d': 8, 'n_a': 16, 'n_steps': 5, 'gamma': 1.4}. Best is trial 0 with value: 0.010426763997343673.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Early stopping occurred at epoch 41 with best_epoch = 31 and best_val_0_rmse = 0.00838\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "[I 2025-05-23 17:29:04,354] Trial 1 finished with value: 0.00837982306548217 and parameters: {'n_d': 16, 'n_a': 8, 'n_steps': 3, 'gamma': 1.4}. Best is trial 1 with value: 0.00837982306548217.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Early stopping occurred at epoch 36 with best_epoch = 26 and best_val_0_rmse = 0.00886\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "[I 2025-05-23 17:43:31,484] Trial 2 finished with value: 0.00885509172971668 and parameters: {'n_d': 24, 'n_a': 16, 'n_steps': 5, 'gamma': 1.4}. Best is trial 1 with value: 0.00837982306548217.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Early stopping occurred at epoch 23 with best_epoch = 13 and best_val_0_rmse = 0.00969\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "[I 2025-05-23 17:50:16,657] Trial 3 finished with value: 0.009691519648185942 and parameters: {'n_d': 16, 'n_a': 16, 'n_steps': 3, 'gamma': 1.4}. Best is trial 1 with value: 0.00837982306548217.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Early stopping occurred at epoch 32 with best_epoch = 22 and best_val_0_rmse = 0.01005\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "[I 2025-05-23 18:03:07,850] Trial 4 finished with value: 0.010054878763500759 and parameters: {'n_d': 16, 'n_a': 24, 'n_steps': 5, 'gamma': 1.4}. Best is trial 1 with value: 0.00837982306548217.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Best RMSE: 0.00837982306548217\n",
            "âœ… Best Params: {'n_d': 16, 'n_a': 8, 'n_steps': 3, 'gamma': 1.4}\n"
          ]
        }
      ],
      "source": [
        "import optuna\n",
        "from pytorch_tabnet.tab_model import TabNetRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "def objective(trial):\n",
        "    model = TabNetRegressor(\n",
        "        n_d=trial.suggest_categorical(\"n_d\", [8, 16, 24]), # ë””ì½”ë” ì°¨ì› ìˆ˜\n",
        "        n_a=trial.suggest_categorical(\"n_a\", [8, 16, 24]), #  ì–´í…ì…˜ í”¼ì²˜ ì°¨ì› ìˆ˜\n",
        "        n_steps=trial.suggest_categorical(\"n_steps\", [3, 5]), # ê²°ì • ë°˜ë³µ íšŸìˆ˜-> í•˜ë‚˜ì˜ ì…ë ¥ì„ ì—¬ëŸ¬ë²ˆ í•™ìŠµ\n",
        "        gamma=trial.suggest_float(\"gamma\", 1.2, 1.5, step=0.1),  # ì´ì „ ì •ë³´ë¥¼ ì–¼ë§ˆë‚˜ í™œìš©í• ì§€ ê²°ì •\n",
        "        lambda_sparse=1e-3,  # í¬ì†Œì„± ì •ê·œí™” ê°•ë„ ê³ ì • L1ê³¼ ìœ ì‚¬í•œ ì—­í• \n",
        "        optimizer_params=dict(lr=0.02),\n",
        "        verbose=0,\n",
        "        seed=42\n",
        "    )\n",
        "\n",
        "    model.fit(\n",
        "        X_train=X_train.values,\n",
        "        y_train=y_train,\n",
        "        eval_set=[(X_valid.values, y_valid)], #í‰ê°€ìš© set\n",
        "        eval_metric=['rmse'],\n",
        "        max_epochs=100,\n",
        "        patience=10, # ì¡°ê¸°ì¢…ë£Œ ì¡°ê±´ 10ë²ˆë™ì•ˆ ì„±ëŠ¥ ì•ˆì˜¤ë¥¼ ì‹œ ì¢…ë£Œ\n",
        "        batch_size=1024,\n",
        "        virtual_batch_size=128\n",
        "    )\n",
        "\n",
        "    preds = model.predict(X_valid.values).squeeze()\n",
        "    mse = mean_squared_error(y_valid, preds)\n",
        "    rmse = np.sqrt(mse)\n",
        "    return rmse\n",
        "\n",
        "study = optuna.create_study(direction=\"minimize\")\n",
        "study.optimize(objective, n_trials=5)  # ê°€ë³ê²Œ 5íšŒë§Œ íƒìƒ‰\n",
        "\n",
        "print(\"âœ… Best RMSE:\", study.best_value)\n",
        "print(\"âœ… Best Params:\", study.best_params)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Wr3AifODZhBv",
      "metadata": {
        "id": "Wr3AifODZhBv"
      },
      "source": [
        "í•™ìŠµ ì‹œê°„ ìƒ ì¤‘ë‹¨, ìœ„ì—ì„œ ë‚˜ì˜¨ ì œì¼ ì¢‹ì€ ê²°ê³¼ë¡œ, í•˜ì´í¼ íŒŒë¼ë¯¸í„° íƒìƒ‰"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71_8s7T33EgS",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 740
        },
        "id": "71_8s7T33EgS",
        "outputId": "8b949bd4-de50-43b0-e402-3a40727804f0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ğŸ” TabNet Tuning Progress:   0%|          | 0/5 [00:00<?, ?it/s][I 2025-05-24 08:33:43,513] A new study created in memory with name: no-name-db6a9744-60ac-4d03-9cc1-d81cab9868f2\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Stop training because you reached max_epochs = 100 with best_epoch = 93 and best_val_0_rmse = 0.00776\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\jeongbin\\Projects\\LandValue_project\\venv\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "ğŸ” TabNet Tuning Progress:  20%|â–ˆâ–ˆ        | 1/5 [46:14<3:04:59, 2774.86s/it][I 2025-05-24 09:19:58,373] Trial 0 finished with value: 0.007758764285731629 and parameters: {'lambda_sparse': 5.314882200191176e-05, 'lr': 0.0018811231971805286, 'batch_size': 1024, 'virtual_batch_size': 256}. Best is trial 0 with value: 0.007758764285731629.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Early stopping occurred at epoch 30 with best_epoch = 20 and best_val_0_rmse = 0.01219\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\jeongbin\\Projects\\LandValue_project\\venv\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "ğŸ” TabNet Tuning Progress:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [1:01:24<1:23:53, 1677.88s/it][I 2025-05-24 09:35:08,368] Trial 1 finished with value: 0.012190128409198812 and parameters: {'lambda_sparse': 0.008379929965598122, 'lr': 0.025442871494994544, 'batch_size': 1024, 'virtual_batch_size': 256}. Best is trial 0 with value: 0.007758764285731629.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Early stopping occurred at epoch 35 with best_epoch = 25 and best_val_0_rmse = 0.00841\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\jeongbin\\Projects\\LandValue_project\\venv\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "ğŸ” TabNet Tuning Progress:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [1:18:55<46:22, 1391.34s/it]  [I 2025-05-24 09:52:38,734] Trial 2 finished with value: 0.008407873176965461 and parameters: {'lambda_sparse': 0.0001409525021766939, 'lr': 0.08670183254700944, 'batch_size': 1024, 'virtual_batch_size': 256}. Best is trial 0 with value: 0.007758764285731629.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Early stopping occurred at epoch 49 with best_epoch = 39 and best_val_0_rmse = 0.00826\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\jeongbin\\Projects\\LandValue_project\\venv\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "ğŸ” TabNet Tuning Progress:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [1:41:45<23:02, 1382.93s/it][I 2025-05-24 10:15:28,766] Trial 3 finished with value: 0.008255294904367792 and parameters: {'lambda_sparse': 2.8736561718466345e-05, 'lr': 0.09718973964557401, 'batch_size': 2048, 'virtual_batch_size': 128}. Best is trial 0 with value: 0.007758764285731629.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Stop training because you reached max_epochs = 100 with best_epoch = 98 and best_val_0_rmse = 0.01224\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\jeongbin\\Projects\\LandValue_project\\venv\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "[W 2025-05-24 11:10:47,360] Trial 4 failed with parameters: {'lambda_sparse': 5.4685178076795676e-05, 'lr': 0.00027093081582010576, 'batch_size': 2048, 'virtual_batch_size': 64} because of the following error: MemoryError((278528, 89), dtype('float64')).\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\jeongbin\\Projects\\LandValue_project\\venv\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 197, in _run_trial\n",
            "    value_or_values = func(trial)\n",
            "  File \"C:\\Users\\jeongbin\\AppData\\Local\\Temp\\ipykernel_4760\\550073449.py\", line 27, in objective_hyper\n",
            "    model.fit(\n",
            "  File \"c:\\Users\\jeongbin\\Projects\\LandValue_project\\venv\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py\", line 278, in fit\n",
            "    self.feature_importances_ = self._compute_feature_importances(X_train)\n",
            "  File \"c:\\Users\\jeongbin\\Projects\\LandValue_project\\venv\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py\", line 759, in _compute_feature_importances\n",
            "    M_explain, _ = self.explain(X, normalize=False)\n",
            "  File \"c:\\Users\\jeongbin\\Projects\\LandValue_project\\venv\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py\", line 369, in explain\n",
            "    res_masks[key] = np.vstack([res_masks[key], value])\n",
            "  File \"c:\\Users\\jeongbin\\Projects\\LandValue_project\\venv\\lib\\site-packages\\numpy\\_core\\shape_base.py\", line 292, in vstack\n",
            "    return _nx.concatenate(arrs, 0, dtype=dtype, casting=casting)\n",
            "numpy._core._exceptions._ArrayMemoryError: Unable to allocate 189. MiB for an array with shape (278528, 89) and data type float64\n",
            "[W 2025-05-24 11:10:47,372] Trial 4 failed with value None.\n"
          ]
        },
        {
          "ename": "MemoryError",
          "evalue": "Unable to allocate 189. MiB for an array with shape (278528, 89) and data type float64",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[3], line 49\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# Optuna íŠœë‹ ì‹¤í–‰\u001b[39;00m\n\u001b[0;32m     48\u001b[0m study_hyper \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mminimize\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 49\u001b[0m \u001b[43mstudy_hyper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective_hyper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNUM_TRIALS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# ê²°ê³¼ ì¶œë ¥\u001b[39;00m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mğŸ“Œ Best RMSE:\u001b[39m\u001b[38;5;124m\"\u001b[39m, study_hyper\u001b[38;5;241m.\u001b[39mbest_value)\n",
            "File \u001b[1;32mc:\\Users\\jeongbin\\Projects\\LandValue_project\\venv\\lib\\site-packages\\optuna\\study\\study.py:475\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    373\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21moptimize\u001b[39m(\n\u001b[0;32m    374\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    375\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    382\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    383\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    384\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    385\u001b[0m \n\u001b[0;32m    386\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    473\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    474\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 475\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    476\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    477\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    478\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    479\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    480\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    481\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    482\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    485\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\jeongbin\\Projects\\LandValue_project\\venv\\lib\\site-packages\\optuna\\study\\_optimize.py:63\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 63\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     76\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
            "File \u001b[1;32mc:\\Users\\jeongbin\\Projects\\LandValue_project\\venv\\lib\\site-packages\\optuna\\study\\_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    157\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 160\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
            "File \u001b[1;32mc:\\Users\\jeongbin\\Projects\\LandValue_project\\venv\\lib\\site-packages\\optuna\\study\\_optimize.py:248\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    241\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    244\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[0;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    246\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    247\u001b[0m ):\n\u001b[1;32m--> 248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
            "File \u001b[1;32mc:\\Users\\jeongbin\\Projects\\LandValue_project\\venv\\lib\\site-packages\\optuna\\study\\_optimize.py:197\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 197\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    198\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    199\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    200\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
            "Cell \u001b[1;32mIn[3], line 27\u001b[0m, in \u001b[0;36mobjective_hyper\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mobjective_hyper\u001b[39m(trial):\n\u001b[0;32m     13\u001b[0m     model \u001b[38;5;241m=\u001b[39m TabNetRegressor(\n\u001b[0;32m     14\u001b[0m         n_d\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m,  \u001b[38;5;66;03m# ê³ ì •\u001b[39;00m\n\u001b[0;32m     15\u001b[0m         n_a\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m,   \u001b[38;5;66;03m# ê³ ì •\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     24\u001b[0m         device_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     25\u001b[0m     )\n\u001b[1;32m---> 27\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_valid\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_valid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrmse\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msuggest_categorical\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbatch_size\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2048\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvirtual_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msuggest_categorical\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvirtual_batch_size\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m     preds \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_valid\u001b[38;5;241m.\u001b[39mvalues)\u001b[38;5;241m.\u001b[39msqueeze()  \u001b[38;5;66;03m# << ë¨¼ì € ì˜ˆì¸¡\u001b[39;00m\n\u001b[0;32m     39\u001b[0m     mse \u001b[38;5;241m=\u001b[39m mean_squared_error(y_valid, preds)\n",
            "File \u001b[1;32mc:\\Users\\jeongbin\\Projects\\LandValue_project\\venv\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:278\u001b[0m, in \u001b[0;36mTabModel.fit\u001b[1;34m(self, X_train, y_train, eval_set, eval_name, eval_metric, loss_fn, weights, max_epochs, patience, batch_size, virtual_batch_size, num_workers, drop_last, callbacks, pin_memory, from_unsupervised, warm_start, augmentations, compute_importance)\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnetwork\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m    276\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_importance:\n\u001b[0;32m    277\u001b[0m     \u001b[38;5;66;03m# compute feature importance once the best model is defined\u001b[39;00m\n\u001b[1;32m--> 278\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_importances_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compute_feature_importances\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\jeongbin\\Projects\\LandValue_project\\venv\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:759\u001b[0m, in \u001b[0;36mTabModel._compute_feature_importances\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    750\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_compute_feature_importances\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m    751\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute global feature importance.\u001b[39;00m\n\u001b[0;32m    752\u001b[0m \n\u001b[0;32m    753\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    757\u001b[0m \n\u001b[0;32m    758\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 759\u001b[0m     M_explain, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexplain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    760\u001b[0m     sum_explain \u001b[38;5;241m=\u001b[39m M_explain\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    761\u001b[0m     feature_importances_ \u001b[38;5;241m=\u001b[39m sum_explain \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39msum(sum_explain)\n",
            "File \u001b[1;32mc:\\Users\\jeongbin\\Projects\\LandValue_project\\venv\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:369\u001b[0m, in \u001b[0;36mTabModel.explain\u001b[1;34m(self, X, normalize)\u001b[0m\n\u001b[0;32m    367\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    368\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m masks\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m--> 369\u001b[0m             res_masks[key] \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mres_masks\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    371\u001b[0m res_explain \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mvstack(res_explain)\n\u001b[0;32m    373\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m normalize:\n",
            "File \u001b[1;32mc:\\Users\\jeongbin\\Projects\\LandValue_project\\venv\\lib\\site-packages\\numpy\\_core\\shape_base.py:292\u001b[0m, in \u001b[0;36mvstack\u001b[1;34m(tup, dtype, casting)\u001b[0m\n\u001b[0;32m    290\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arrs, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    291\u001b[0m     arrs \u001b[38;5;241m=\u001b[39m (arrs,)\n\u001b[1;32m--> 292\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_nx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcasting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcasting\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 189. MiB for an array with shape (278528, 89) and data type float64"
          ]
        }
      ],
      "source": [
        "# í•˜ì´í¼ íŒŒë¼ë¯¸í„° íƒìƒ‰\n",
        "\n",
        "from pytorch_tabnet.tab_model import TabNetRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import optuna\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "# tqdm ì§„í–‰ë¥  ë°”\n",
        "NUM_TRIALS = 5\n",
        "pbar = tqdm(total=NUM_TRIALS, desc=\"ğŸ” TabNet Tuning Progress\")\n",
        "\n",
        "def objective_hyper(trial):\n",
        "    model = TabNetRegressor(\n",
        "        n_d=16,  # ê³ ì •\n",
        "        n_a=8,   # ê³ ì •\n",
        "        n_steps=3,  # ê³ ì •\n",
        "        gamma=1.4,  # ê³ ì •\n",
        "        lambda_sparse=trial.suggest_float(\"lambda_sparse\", 1e-5, 1e-2, log=True),\n",
        "        optimizer_params=dict(\n",
        "            lr=trial.suggest_float(\"lr\", 1e-4, 1e-1, log=True)\n",
        "        ),\n",
        "        verbose=0,\n",
        "        seed=42,\n",
        "        device_name=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    )\n",
        "\n",
        "    model.fit(\n",
        "        X_train=X_train.values,\n",
        "        y_train=y_train,\n",
        "        eval_set=[(X_valid.values, y_valid)],\n",
        "        eval_metric=['rmse'],\n",
        "        max_epochs=100,\n",
        "        patience=10,\n",
        "        batch_size=trial.suggest_categorical(\"batch_size\", [1024, 2048]),\n",
        "        virtual_batch_size=trial.suggest_categorical(\"virtual_batch_size\", [64, 128,256])\n",
        "        compute_importance=False\n",
        "    )\n",
        "\n",
        "    preds = model.predict(X_valid.values).squeeze()  # << ë¨¼ì € ì˜ˆì¸¡\n",
        "    mse = mean_squared_error(y_valid, preds)\n",
        "    rmse = np.sqrt(mse)\n",
        "    r2 = r2_score(y_valid, preds)\n",
        "\n",
        "    trial.set_user_attr(\"r2\", r2)\n",
        "    pbar.update(1)\n",
        "    return rmse\n",
        "\n",
        "# Optuna íŠœë‹ ì‹¤í–‰\n",
        "study_hyper = optuna.create_study(direction=\"minimize\")\n",
        "study_hyper.optimize(objective_hyper, n_trials=NUM_TRIALS)\n",
        "\n",
        "# ê²°ê³¼ ì¶œë ¥\n",
        "print(\"ğŸ“Œ Best RMSE:\", study_hyper.best_value)\n",
        "print(\"ğŸ“Œ Best Hyperparams:\", study_hyper.best_params)\n",
        "\n",
        "best_trial = study_hyper.best_trial\n",
        "print(\"ğŸ“ˆ Best RÂ²:\", best_trial.user_attrs.get(\"r2\", \"N/A\"))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c123e57f",
      "metadata": {
        "id": "c123e57f",
        "outputId": "724c3cb1-1d03-4a73-bebc-8af6d00ebde3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ“Œ Best RMSE: 0.007758764285731629\n",
            "ğŸ“Œ Best Hyperparams: {'lambda_sparse': 5.314882200191176e-05, 'lr': 0.0018811231971805286, 'batch_size': 1024, 'virtual_batch_size': 256}\n",
            "ğŸ“ˆ Best RÂ²: 0.7412711709293286\n"
          ]
        }
      ],
      "source": [
        "# ê²°ê³¼ ì¶œë ¥\n",
        "print(\"ğŸ“Œ Best RMSE:\", study_hyper.best_value)\n",
        "print(\"ğŸ“Œ Best Hyperparams:\", study_hyper.best_params)\n",
        "\n",
        "best_trial = study_hyper.best_trial\n",
        "print(\"ğŸ“ˆ Best RÂ²:\", best_trial.user_attrs.get(\"r2\", \"N/A\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f800d65b",
      "metadata": {
        "id": "f800d65b"
      },
      "source": [
        "### ë©”ëª¨ë¦¬ ë¶€ì¡±ìœ¼ë¡œ ì¸í•´ 5íšŒê°€ ì•„ë‹Œ 4íšŒë™ì•ˆ ê¸°ë¡ëœ ìµœì ê°’ì„ ì´ìš©í•´ í•™ìŠµ ì§„í–‰"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17ec1770",
      "metadata": {
        "id": "17ec1770",
        "outputId": "6caede03-c616-4c3a-b2be-9119b5da8e6f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 0  | loss: 0.36183 | val_0_rmse: 0.04153 |  0:00:27s\n",
            "epoch 1  | loss: 0.00359 | val_0_rmse: 0.03013 |  0:00:52s\n",
            "epoch 2  | loss: 0.00152 | val_0_rmse: 0.02295 |  0:01:21s\n",
            "epoch 3  | loss: 0.00093 | val_0_rmse: 0.01927 |  0:01:50s\n",
            "epoch 4  | loss: 0.00063 | val_0_rmse: 0.01751 |  0:02:20s\n",
            "epoch 5  | loss: 0.00049 | val_0_rmse: 0.01618 |  0:02:48s\n",
            "epoch 6  | loss: 0.0004  | val_0_rmse: 0.01563 |  0:03:18s\n",
            "epoch 7  | loss: 0.00037 | val_0_rmse: 0.0147  |  0:03:47s\n",
            "epoch 8  | loss: 0.00033 | val_0_rmse: 0.01432 |  0:04:14s\n",
            "epoch 9  | loss: 0.00031 | val_0_rmse: 0.014   |  0:04:43s\n",
            "epoch 10 | loss: 0.00029 | val_0_rmse: 0.01378 |  0:05:10s\n",
            "epoch 11 | loss: 0.00028 | val_0_rmse: 0.01378 |  0:05:39s\n",
            "epoch 12 | loss: 0.00027 | val_0_rmse: 0.01364 |  0:06:07s\n",
            "epoch 13 | loss: 0.00027 | val_0_rmse: 0.0135  |  0:06:35s\n",
            "epoch 14 | loss: 0.00026 | val_0_rmse: 0.01349 |  0:07:02s\n",
            "epoch 15 | loss: 0.00026 | val_0_rmse: 0.0135  |  0:07:31s\n",
            "epoch 16 | loss: 0.00026 | val_0_rmse: 0.01353 |  0:08:01s\n",
            "epoch 17 | loss: 0.00025 | val_0_rmse: 0.01332 |  0:08:31s\n",
            "epoch 18 | loss: 0.00025 | val_0_rmse: 0.01326 |  0:08:59s\n",
            "epoch 19 | loss: 0.00024 | val_0_rmse: 0.01273 |  0:09:27s\n",
            "epoch 20 | loss: 0.00023 | val_0_rmse: 0.01253 |  0:09:58s\n",
            "epoch 21 | loss: 0.00023 | val_0_rmse: 0.01226 |  0:10:26s\n",
            "epoch 22 | loss: 0.00022 | val_0_rmse: 0.01248 |  0:10:55s\n",
            "epoch 23 | loss: 0.00021 | val_0_rmse: 0.01178 |  0:11:23s\n",
            "epoch 24 | loss: 0.00021 | val_0_rmse: 0.01173 |  0:11:50s\n",
            "epoch 25 | loss: 0.00021 | val_0_rmse: 0.01183 |  0:12:19s\n",
            "epoch 26 | loss: 0.0002  | val_0_rmse: 0.01152 |  0:12:47s\n",
            "epoch 27 | loss: 0.00019 | val_0_rmse: 0.01116 |  0:13:17s\n",
            "epoch 28 | loss: 0.00019 | val_0_rmse: 0.01078 |  0:13:45s\n",
            "epoch 29 | loss: 0.00018 | val_0_rmse: 0.01051 |  0:14:13s\n",
            "epoch 30 | loss: 0.00017 | val_0_rmse: 0.01028 |  0:14:41s\n",
            "epoch 31 | loss: 0.00016 | val_0_rmse: 0.01012 |  0:15:13s\n",
            "epoch 32 | loss: 0.00016 | val_0_rmse: 0.01016 |  0:15:42s\n",
            "epoch 33 | loss: 0.00015 | val_0_rmse: 0.01023 |  0:16:10s\n",
            "epoch 34 | loss: 0.00015 | val_0_rmse: 0.01006 |  0:16:41s\n",
            "epoch 35 | loss: 0.00015 | val_0_rmse: 0.00974 |  0:17:15s\n",
            "epoch 36 | loss: 0.00014 | val_0_rmse: 0.00965 |  0:17:51s\n",
            "epoch 37 | loss: 0.00014 | val_0_rmse: 0.00988 |  0:18:26s\n",
            "epoch 38 | loss: 0.00013 | val_0_rmse: 0.00974 |  0:19:10s\n",
            "epoch 39 | loss: 0.00013 | val_0_rmse: 0.00988 |  0:19:42s\n",
            "epoch 40 | loss: 0.00013 | val_0_rmse: 0.00933 |  0:20:20s\n",
            "epoch 41 | loss: 0.00012 | val_0_rmse: 0.00916 |  0:20:55s\n",
            "epoch 42 | loss: 0.00012 | val_0_rmse: 0.009   |  0:21:34s\n",
            "epoch 43 | loss: 0.00011 | val_0_rmse: 0.00885 |  0:22:14s\n",
            "epoch 44 | loss: 0.00011 | val_0_rmse: 0.00871 |  0:23:05s\n",
            "epoch 45 | loss: 0.00011 | val_0_rmse: 0.00895 |  0:23:46s\n",
            "epoch 46 | loss: 0.0001  | val_0_rmse: 0.00875 |  0:24:25s\n",
            "epoch 47 | loss: 0.0001  | val_0_rmse: 0.00868 |  0:24:58s\n",
            "epoch 48 | loss: 0.0001  | val_0_rmse: 0.00846 |  0:25:25s\n",
            "epoch 49 | loss: 0.0001  | val_0_rmse: 0.00846 |  0:25:52s\n",
            "epoch 50 | loss: 9e-05   | val_0_rmse: 0.00849 |  0:26:19s\n",
            "epoch 51 | loss: 9e-05   | val_0_rmse: 0.00877 |  0:26:46s\n",
            "epoch 52 | loss: 9e-05   | val_0_rmse: 0.00851 |  0:27:14s\n",
            "epoch 53 | loss: 9e-05   | val_0_rmse: 0.00844 |  0:27:41s\n",
            "epoch 54 | loss: 9e-05   | val_0_rmse: 0.00849 |  0:28:08s\n",
            "epoch 55 | loss: 9e-05   | val_0_rmse: 0.00833 |  0:28:35s\n",
            "epoch 56 | loss: 9e-05   | val_0_rmse: 0.00852 |  0:29:02s\n",
            "epoch 57 | loss: 9e-05   | val_0_rmse: 0.00833 |  0:29:29s\n",
            "epoch 58 | loss: 9e-05   | val_0_rmse: 0.00864 |  0:29:57s\n",
            "epoch 59 | loss: 9e-05   | val_0_rmse: 0.00872 |  0:30:24s\n",
            "epoch 60 | loss: 9e-05   | val_0_rmse: 0.00834 |  0:30:51s\n",
            "epoch 61 | loss: 9e-05   | val_0_rmse: 0.00833 |  0:31:18s\n",
            "epoch 62 | loss: 8e-05   | val_0_rmse: 0.00842 |  0:31:44s\n",
            "epoch 63 | loss: 9e-05   | val_0_rmse: 0.00832 |  0:32:11s\n",
            "epoch 64 | loss: 8e-05   | val_0_rmse: 0.00848 |  0:32:39s\n",
            "epoch 65 | loss: 8e-05   | val_0_rmse: 0.00827 |  0:33:06s\n",
            "epoch 66 | loss: 8e-05   | val_0_rmse: 0.00819 |  0:33:33s\n",
            "epoch 67 | loss: 8e-05   | val_0_rmse: 0.00826 |  0:33:59s\n",
            "epoch 68 | loss: 8e-05   | val_0_rmse: 0.00847 |  0:34:26s\n",
            "epoch 69 | loss: 8e-05   | val_0_rmse: 0.00805 |  0:35:26s\n",
            "epoch 70 | loss: 8e-05   | val_0_rmse: 0.00819 |  0:36:07s\n",
            "epoch 71 | loss: 8e-05   | val_0_rmse: 0.00825 |  0:36:35s\n",
            "epoch 72 | loss: 8e-05   | val_0_rmse: 0.00804 |  0:37:01s\n",
            "epoch 73 | loss: 8e-05   | val_0_rmse: 0.00803 |  0:37:29s\n",
            "epoch 74 | loss: 8e-05   | val_0_rmse: 0.00859 |  0:37:56s\n",
            "epoch 75 | loss: 8e-05   | val_0_rmse: 0.00803 |  0:38:26s\n",
            "epoch 76 | loss: 8e-05   | val_0_rmse: 0.00807 |  0:38:53s\n",
            "epoch 77 | loss: 8e-05   | val_0_rmse: 0.00795 |  0:39:20s\n",
            "epoch 78 | loss: 8e-05   | val_0_rmse: 0.00793 |  0:39:47s\n",
            "epoch 79 | loss: 8e-05   | val_0_rmse: 0.00798 |  0:40:20s\n",
            "epoch 80 | loss: 8e-05   | val_0_rmse: 0.00806 |  0:41:08s\n",
            "epoch 81 | loss: 8e-05   | val_0_rmse: 0.00792 |  0:41:41s\n",
            "epoch 82 | loss: 8e-05   | val_0_rmse: 0.00789 |  0:42:18s\n",
            "epoch 83 | loss: 8e-05   | val_0_rmse: 0.00799 |  0:43:08s\n",
            "epoch 84 | loss: 8e-05   | val_0_rmse: 0.00797 |  0:43:40s\n",
            "epoch 85 | loss: 8e-05   | val_0_rmse: 0.00795 |  0:44:07s\n",
            "epoch 86 | loss: 8e-05   | val_0_rmse: 0.00782 |  0:44:34s\n",
            "epoch 87 | loss: 8e-05   | val_0_rmse: 0.00787 |  0:45:02s\n",
            "epoch 88 | loss: 8e-05   | val_0_rmse: 0.0079  |  0:45:29s\n",
            "epoch 89 | loss: 8e-05   | val_0_rmse: 0.00796 |  0:45:57s\n",
            "epoch 90 | loss: 8e-05   | val_0_rmse: 0.00786 |  0:46:24s\n",
            "epoch 91 | loss: 7e-05   | val_0_rmse: 0.00799 |  0:46:51s\n",
            "epoch 92 | loss: 8e-05   | val_0_rmse: 0.00782 |  0:47:17s\n",
            "epoch 93 | loss: 8e-05   | val_0_rmse: 0.00776 |  0:47:45s\n",
            "epoch 94 | loss: 8e-05   | val_0_rmse: 0.00788 |  0:48:12s\n",
            "epoch 95 | loss: 7e-05   | val_0_rmse: 0.0078  |  0:48:39s\n",
            "epoch 96 | loss: 7e-05   | val_0_rmse: 0.00812 |  0:49:09s\n",
            "epoch 97 | loss: 7e-05   | val_0_rmse: 0.00802 |  0:49:42s\n",
            "epoch 98 | loss: 8e-05   | val_0_rmse: 0.0078  |  0:50:13s\n",
            "epoch 99 | loss: 7e-05   | val_0_rmse: 0.00781 |  0:50:39s\n",
            "Stop training because you reached max_epochs = 100 with best_epoch = 93 and best_val_0_rmse = 0.00776\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\jeongbin\\Projects\\LandValue_project\\venv\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… ìµœì¢… RMSE: 1568550.27499\n",
            "âœ… ìµœì¢… RÂ²: 0.65572\n"
          ]
        }
      ],
      "source": [
        "from pytorch_tabnet.tab_model import TabNetRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import torch\n",
        "import numpy as np\n",
        "import os\n",
        "# ì €ì¥ ê²½ë¡œ ì§€ì •\n",
        "model_save_path = \"./best_tabnet_model.zip\"\n",
        "# 1. ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°\n",
        "best_params = {\n",
        "    'lambda_sparse': 5.314882200191176e-05,\n",
        "    'lr': 0.0018811231971805286,\n",
        "    'batch_size': 1024,\n",
        "    'virtual_batch_size': 256\n",
        "}\n",
        "\n",
        "# 2. TabNet ëª¨ë¸ ìƒì„±\n",
        "model = TabNetRegressor(\n",
        "    n_d=16,\n",
        "    n_a=8,\n",
        "    n_steps=3,\n",
        "    gamma=1.4,\n",
        "    lambda_sparse=best_params['lambda_sparse'],\n",
        "    optimizer_params=dict(lr=best_params['lr']),\n",
        "    verbose=1,\n",
        "    seed=42,\n",
        "    device_name=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        ")\n",
        "\n",
        "# 3. í•™ìŠµ\n",
        "model.fit(\n",
        "    X_train=X_train.values.astype(np.float32),  # float32ë¡œ ë³€í™˜í•´ì„œ ë©”ëª¨ë¦¬ ì ˆì•½\n",
        "    y_train=y_train,\n",
        "    eval_set=[(X_valid.values.astype(np.float32), y_valid)],\n",
        "    eval_metric=['rmse'],\n",
        "    max_epochs=100,\n",
        "    patience=10,\n",
        "    batch_size=best_params['batch_size'],\n",
        "    virtual_batch_size=best_params['virtual_batch_size']\n",
        ")\n",
        "\n",
        "# 4. ì˜ˆì¸¡\n",
        "preds = model.predict(X_test.values.astype(np.float32)).squeeze()\n",
        "\n",
        "# 5. ì—­ì •ê·œí™”\n",
        "y_min = 216.0\n",
        "y_max = 165300000.0\n",
        "\n",
        "preds_inverse = preds * (y_max - y_min) + y_min\n",
        "y_test_inverse = y_test * (y_max - y_min) + y_min\n",
        "\n",
        "# ì„±ëŠ¥ í‰ê°€\n",
        "mse = mean_squared_error(y_test_inverse, preds_inverse)\n",
        "rmse = np.sqrt(mse)\n",
        "r2 = r2_score(y_test_inverse, preds_inverse)\n",
        "\n",
        "print(f\"âœ… ìµœì¢… RMSE: {rmse:.5f}\")\n",
        "print(f\"âœ… ìµœì¢… RÂ²: {r2:.5f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "632ea0ac",
      "metadata": {
        "id": "632ea0ac",
        "outputId": "ea2df567-7f62-4705-c10a-c557917c37bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully saved model at best_tabnet_model.zip\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'best_tabnet_model.zip'"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# í•™ìŠµ ì™„ë£Œëœ model ê°ì²´ ì €ì¥\n",
        "model.save_model(\"best_tabnet_model\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da6c6344",
      "metadata": {
        "id": "da6c6344",
        "outputId": "4cb2bb06-3901-4f3f-ac09-241293e90d45"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\jeongbin\\Projects\\LandValue_project\\venv\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.6557154562734671\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "loaded_model = TabNetRegressor()\n",
        "loaded_model.load_model(\"./model/best_tabnet_model.zip\")\n",
        "\n",
        "# ì˜ˆì¸¡ ê°€ëŠ¥\n",
        "preds = loaded_model.predict(X_test.values.astype(np.float32)).squeeze()\n",
        "r2 = r2_score(y_test, preds)\n",
        "print(r2)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f05f8b54",
      "metadata": {
        "id": "f05f8b54"
      },
      "source": [
        "### ft-transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "BzGLDoZ9_rtj",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BzGLDoZ9_rtj",
        "outputId": "2a6aaed0-d5b7-445f-f1da-6103b916c499"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ë³€ìˆ˜ 0 â†’ ê³ ìœ³ê°’ ê°œìˆ˜: 3\n",
            "ë³€ìˆ˜ 1 â†’ ê³ ìœ³ê°’ ê°œìˆ˜: 15\n",
            "ë³€ìˆ˜ 2 â†’ ê³ ìœ³ê°’ ê°œìˆ˜: 7\n",
            "ë³€ìˆ˜ 3 â†’ ê³ ìœ³ê°’ ê°œìˆ˜: 30954\n",
            "ë³€ìˆ˜ 4 â†’ ê³ ìœ³ê°’ ê°œìˆ˜: 14\n",
            "ë³€ìˆ˜ 5 â†’ ê³ ìœ³ê°’ ê°œìˆ˜: 9\n",
            "ë³€ìˆ˜ 6 â†’ ê³ ìœ³ê°’ ê°œìˆ˜: 6\n",
            "ë³€ìˆ˜ 7 â†’ ê³ ìœ³ê°’ ê°œìˆ˜: 4\n",
            "ë³€ìˆ˜ 8 â†’ ê³ ìœ³ê°’ ê°œìˆ˜: 6\n",
            "ë³€ìˆ˜ 9 â†’ ê³ ìœ³ê°’ ê°œìˆ˜: 12\n",
            "ë³€ìˆ˜ 10 â†’ ê³ ìœ³ê°’ ê°œìˆ˜: 6\n",
            "ë³€ìˆ˜ 11 â†’ ê³ ìœ³ê°’ ê°œìˆ˜: 3\n",
            "ë³€ìˆ˜ 12 â†’ ê³ ìœ³ê°’ ê°œìˆ˜: 12\n",
            "ë³€ìˆ˜ 13 â†’ ê³ ìœ³ê°’ ê°œìˆ˜: 2\n",
            "ë³€ìˆ˜ 14 â†’ ê³ ìœ³ê°’ ê°œìˆ˜: 3\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import pickle\n",
        "# âœ… ë°ì´í„° ë¡œë”©\n",
        "with open(\"split_data_ft_transformer.pkl\", \"rb\") as f:\n",
        "    X_train_tensor, X_valid_tensor, X_test_tensor, y_train_tensor, y_valid_tensor, y_test_tensor = pickle.load(f)\n",
        "# í…ì„œë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜\n",
        "df = pd.DataFrame(X_train_tensor.numpy())\n",
        "\n",
        "# ê° ì—´ë§ˆë‹¤ ê³ ìœ³ê°’ ê°œìˆ˜ ì¶œë ¥\n",
        "for col in df.columns:\n",
        "    unique_count = df[col].nunique()\n",
        "    print(f\"ë³€ìˆ˜ {col} â†’ ê³ ìœ³ê°’ ê°œìˆ˜: {unique_count}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "KCXzq-Q4BgvK",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KCXzq-Q4BgvK",
        "outputId": "6f77b5a8-6d1c-4953-b794-b50ec0052c8b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-06-07 15:50:59,845] A new study created in memory with name: no-name-7a1507ff-9524-4f09-b93a-4808e4cafb01\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-06-07 16:13:11,165] Trial 0 finished with value: 0.007743737660348415 and parameters: {'d_token': 128, 'n_blocks': 4, 'n_heads': 2, 'dropout': 0.33091364406781176, 'ffn_d_hidden': 512}. Best is trial 0 with value: 0.007743737660348415.\n",
            "[I 2025-06-07 16:23:22,891] Trial 1 finished with value: 0.00785342138260603 and parameters: {'d_token': 32, 'n_blocks': 4, 'n_heads': 4, 'dropout': 0.433636849327123, 'ffn_d_hidden': 128}. Best is trial 0 with value: 0.007743737660348415.\n",
            "[I 2025-06-07 16:29:13,561] Trial 2 finished with value: 0.008003488183021545 and parameters: {'d_token': 64, 'n_blocks': 3, 'n_heads': 4, 'dropout': 0.4855313235003095, 'ffn_d_hidden': 128}. Best is trial 0 with value: 0.007743737660348415.\n",
            "[I 2025-06-07 16:37:35,607] Trial 3 finished with value: 0.007921209558844566 and parameters: {'d_token': 32, 'n_blocks': 4, 'n_heads': 2, 'dropout': 0.4374995274986443, 'ffn_d_hidden': 128}. Best is trial 0 with value: 0.007743737660348415.\n",
            "[I 2025-06-07 17:09:34,092] Trial 4 finished with value: 0.007526722736656666 and parameters: {'d_token': 128, 'n_blocks': 5, 'n_heads': 2, 'dropout': 0.1476045541216017, 'ffn_d_hidden': 128}. Best is trial 4 with value: 0.007526722736656666.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best trial:\n",
            "FrozenTrial(number=4, state=1, values=[0.007526722736656666], datetime_start=datetime.datetime(2025, 6, 7, 16, 37, 35, 607975), datetime_complete=datetime.datetime(2025, 6, 7, 17, 9, 34, 92038), params={'d_token': 128, 'n_blocks': 5, 'n_heads': 2, 'dropout': 0.1476045541216017, 'ffn_d_hidden': 128}, user_attrs={}, system_attrs={}, intermediate_values={}, distributions={'d_token': CategoricalDistribution(choices=(32, 64, 128)), 'n_blocks': IntDistribution(high=6, log=False, low=2, step=1), 'n_heads': CategoricalDistribution(choices=(2, 4, 8)), 'dropout': FloatDistribution(high=0.5, log=False, low=0.1, step=None), 'ffn_d_hidden': CategoricalDistribution(choices=(128, 256, 512))}, trial_id=4, value=None)\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from rtdl import FTTransformer\n",
        "import optuna\n",
        "\n",
        "# âœ… GPU ì„¤ì • (ì½”ë©ì—ì„œ ì§„í–‰)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# âœ… ë°ì´í„° ë¡œë”©\n",
        "with open(\"split_data_ft_transformer.pkl\", \"rb\") as f:\n",
        "    X_train_tensor, X_valid_tensor, X_test_tensor, y_train_tensor, y_valid_tensor, y_test_tensor = pickle.load(f)\n",
        "\n",
        "# âœ… ì—°ì†í˜•: 3ë²ˆ ì¸ë±ìŠ¤ë§Œ / ë‚˜ë¨¸ì§€ëŠ” ë²”ì£¼í˜•\n",
        "num_indices = [3]\n",
        "cat_indices = [i for i in range(X_train_tensor.shape[1]) if i != 3]\n",
        "\n",
        "# âœ… ë²”ì£¼í˜• ë³€ìˆ˜ ê³ ìœ ê°’ ê°œìˆ˜\n",
        "cat_cardinalities = [\n",
        "    int(X_train_tensor[:, i].max().item()) + 1\n",
        "    for i in cat_indices\n",
        "]\n",
        "\n",
        "# âœ… ì…ë ¥ ë¶„ë¦¬ í•¨ìˆ˜\n",
        "def split_features(X_tensor):\n",
        "    x_num = X_tensor[:, num_indices].float()\n",
        "    x_cat = X_tensor[:, cat_indices].long()  #  long() ì¶”ê°€\n",
        "    return x_num, x_cat\n",
        "\n",
        "# âœ… DataLoader êµ¬ì„±\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "valid_dataset = TensorDataset(X_valid_tensor, y_valid_tensor)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=512, shuffle=False)\n",
        "\n",
        "# âœ… Optuna ëª©ì  í•¨ìˆ˜\n",
        "def objective(trial):\n",
        "    # í•˜ì´í¼íŒŒë¼ë¯¸í„° ìƒ˜í”Œë§\n",
        "    d_token = trial.suggest_categorical(\"d_token\", [32, 64, 128])\n",
        "    n_blocks = trial.suggest_int(\"n_blocks\", 2, 6)\n",
        "    n_heads = trial.suggest_categorical(\"n_heads\", [2, 4, 8])\n",
        "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
        "    ffn_d_hidden = trial.suggest_categorical(\"ffn_d_hidden\", [128, 256, 512])\n",
        "\n",
        "    # âœ… ëª¨ë¸ ì •ì˜\n",
        "    model = FTTransformer.make_baseline(\n",
        "        n_num_features=len(num_indices),\n",
        "        cat_cardinalities=cat_cardinalities,\n",
        "        d_token=d_token,\n",
        "        n_blocks=n_blocks,\n",
        "        attention_dropout=dropout,\n",
        "        ffn_d_hidden=ffn_d_hidden,\n",
        "        ffn_dropout=dropout,\n",
        "        residual_dropout=dropout,\n",
        "        d_out=1\n",
        "    ).to(device)\n",
        "\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "    loss_fn = nn.MSELoss()\n",
        "\n",
        "    best_rmse = float(\"inf\")\n",
        "    patience = 10\n",
        "    trigger_times = 0\n",
        "\n",
        "    for epoch in range(80):\n",
        "        model.train()\n",
        "        for xb, yb in train_loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            x_num, x_cat = split_features(xb)\n",
        "            x_num, x_cat = x_num.to(device), x_cat.to(device)\n",
        "            preds = model(x_num, x_cat)\n",
        "            loss = loss_fn(preds, yb)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # âœ… ê²€ì¦\n",
        "        model.eval()\n",
        "        preds = []\n",
        "        with torch.no_grad():\n",
        "            for xb, _ in valid_loader:\n",
        "                xb = xb.to(device)\n",
        "                x_num, x_cat = split_features(xb)\n",
        "                pred = model(x_num, x_cat)\n",
        "                preds.append(pred.cpu().numpy())\n",
        "\n",
        "        preds = np.vstack(preds)\n",
        "        rmse = np.sqrt(np.mean((y_valid_tensor.numpy() - preds) ** 2))\n",
        "\n",
        "        # âœ… ì¡°ê¸° ì¢…ë£Œ\n",
        "        if rmse < best_rmse:\n",
        "            best_rmse = rmse\n",
        "            trigger_times = 0\n",
        "        else:\n",
        "            trigger_times += 1\n",
        "            if trigger_times >= patience:\n",
        "                break\n",
        "\n",
        "    return best_rmse\n",
        "\n",
        "# âœ… Optuna ì‹¤í–‰\n",
        "study = optuna.create_study(direction=\"minimize\")\n",
        "study.optimize(objective, n_trials=5)\n",
        "\n",
        "# âœ… ê²°ê³¼ ì¶œë ¥\n",
        "print(\"Best trial:\")\n",
        "print(study.best_trial)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "rfmAoh8vL3B_",
      "metadata": {
        "id": "rfmAoh8vL3B_"
      },
      "outputs": [],
      "source": [
        "fixed_params = {\n",
        "    \"d_token\": 128,\n",
        "    \"n_blocks\": 5,\n",
        "    \"n_heads\": 2,\n",
        "    \"dropout\": 0.1476045541216017,\n",
        "    \"ffn_d_hidden\": 128\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "uou0Of15YdBz",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uou0Of15YdBz",
        "outputId": "396b6e53-68a1-421e-dd14-e71476a1119f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-06-08 03:32:22,033] A new study created in memory with name: no-name-d7c84442-651f-44f2-9b04-f49075f61191\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-06-08 03:39:35,854] Trial 0 finished with value: 0.00887646246701479 and parameters: {'lr': 0.00018880225028845442, 'weight_decay': 0.009014651901957674, 'batch_size': 2048}. Best is trial 0 with value: 0.00887646246701479.\n",
            "[I 2025-06-08 03:51:07,795] Trial 1 finished with value: 0.008109040558338165 and parameters: {'lr': 0.00020817657634782943, 'weight_decay': 0.0064234854267522035, 'batch_size': 1024}. Best is trial 1 with value: 0.008109040558338165.\n",
            "[I 2025-06-08 03:59:01,229] Trial 2 finished with value: 0.009245762601494789 and parameters: {'lr': 0.00036692557357585726, 'weight_decay': 0.0006471050252809052, 'batch_size': 2048}. Best is trial 1 with value: 0.008109040558338165.\n",
            "[I 2025-06-08 04:13:03,823] Trial 3 finished with value: 0.007885748520493507 and parameters: {'lr': 0.001569851841594099, 'weight_decay': 0.004428466921695153, 'batch_size': 1024}. Best is trial 3 with value: 0.007885748520493507.\n",
            "[I 2025-06-08 04:27:09,502] Trial 4 finished with value: 0.008108886890113354 and parameters: {'lr': 0.0008102416876887378, 'weight_decay': 0.008164775157127473, 'batch_size': 2048}. Best is trial 3 with value: 0.007885748520493507.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best trial:\n",
            "FrozenTrial(number=3, state=1, values=[0.007885748520493507], datetime_start=datetime.datetime(2025, 6, 8, 3, 59, 1, 230832), datetime_complete=datetime.datetime(2025, 6, 8, 4, 13, 3, 823236), params={'lr': 0.001569851841594099, 'weight_decay': 0.004428466921695153, 'batch_size': 1024}, user_attrs={}, system_attrs={}, intermediate_values={}, distributions={'lr': FloatDistribution(high=0.003, log=True, low=0.0001, step=None), 'weight_decay': FloatDistribution(high=0.01, log=False, low=0.0, step=None), 'batch_size': CategoricalDistribution(choices=(512, 1024, 2048))}, trial_id=3, value=None)\n"
          ]
        }
      ],
      "source": [
        "import optuna\n",
        "import pickle\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from rtdl import FTTransformer\n",
        "import optuna\n",
        "# âœ… GPU ì„¤ì • (ì½”ë©ì—ì„œ ì§„í–‰)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# âœ… ë°ì´í„° ë¡œë”©\n",
        "with open(\"split_data_ft_transformer.pkl\", \"rb\") as f:\n",
        "    X_train_tensor, X_valid_tensor, X_test_tensor, y_train_tensor, y_valid_tensor, y_test_tensor = pickle.load(f)\n",
        "\n",
        "num_indices = [3]\n",
        "cat_indices = [i for i in range(X_train_tensor.shape[1]) if i != 3]\n",
        "\n",
        "# âœ… ë²”ì£¼í˜• ë³€ìˆ˜ ê³ ìœ ê°’ ê°œìˆ˜\n",
        "cat_cardinalities = [\n",
        "    int(X_train_tensor[:, i].max().item()) + 1\n",
        "    for i in cat_indices\n",
        "]\n",
        "# âœ… ì…ë ¥ ë¶„ë¦¬ í•¨ìˆ˜\n",
        "def split_features(X_tensor):\n",
        "    x_num = X_tensor[:, num_indices].float()\n",
        "    x_cat = X_tensor[:, cat_indices].long()  #  long() ì¶”ê°€\n",
        "    return x_num, x_cat\n",
        "\n",
        "# âœ… DataLoader êµ¬ì„±\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "valid_dataset = TensorDataset(X_valid_tensor, y_valid_tensor)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=512, shuffle=False)\n",
        "\n",
        "def hparam_objective(trial):\n",
        "    # âœ… íƒìƒ‰ ëŒ€ìƒ í•™ìŠµ í•˜ì´í¼íŒŒë¼ë¯¸í„°\n",
        "    lr = trial.suggest_float(\"lr\", 1e-4, 3e-3, log=True)\n",
        "    weight_decay = trial.suggest_float(\"weight_decay\", 0.0, 1e-2)\n",
        "    batch_size = trial.suggest_categorical(\"batch_size\", [512, 1024, 2048])\n",
        "\n",
        "    # âœ… ê³ ì • êµ¬ì¡° í•˜ì´í¼íŒŒë¼ë¯¸í„°\n",
        "    model = FTTransformer.make_baseline(\n",
        "        n_num_features=len(num_indices),\n",
        "        cat_cardinalities=cat_cardinalities,\n",
        "        d_token=fixed_params[\"d_token\"],\n",
        "        n_blocks=fixed_params[\"n_blocks\"],\n",
        "        attention_dropout=fixed_params[\"dropout\"],\n",
        "        ffn_d_hidden=fixed_params[\"ffn_d_hidden\"],\n",
        "        ffn_dropout=fixed_params[\"dropout\"],\n",
        "        residual_dropout=fixed_params[\"dropout\"],\n",
        "        d_out=1\n",
        "    ).to(device)\n",
        "\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    loss_fn = nn.MSELoss()\n",
        "\n",
        "    # âœ… ë°°ì¹˜ì‚¬ì´ì¦ˆ ë°˜ì˜\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
        "    valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
        "\n",
        "    best_rmse = float(\"inf\")\n",
        "    patience = 10\n",
        "    trigger_times = 0\n",
        "\n",
        "    for epoch in range(80):\n",
        "        model.train()\n",
        "        for xb, yb in train_loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            x_num, x_cat = split_features(xb)  # âœ… ìˆ˜ì •ëœ ë¶€ë¶„\n",
        "            x_num, x_cat = x_num.to(device), x_cat.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            preds = model(x_num, x_cat)  # âœ… ìˆ˜ì •ëœ ë¶€ë¶„\n",
        "            loss = loss_fn(preds, yb)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # âœ… ê²€ì¦\n",
        "        model.eval()\n",
        "        preds = []\n",
        "        with torch.no_grad():\n",
        "            for xb, _ in valid_loader:\n",
        "                xb = xb.to(device)\n",
        "                x_num, x_cat = split_features(xb)  # âœ… ìˆ˜ì •ëœ ë¶€ë¶„\n",
        "                x_num, x_cat = x_num.to(device), x_cat.to(device)\n",
        "\n",
        "                pred = model(x_num, x_cat)  # âœ… ìˆ˜ì •ëœ ë¶€ë¶„\n",
        "                preds.append(pred.cpu().numpy())\n",
        "\n",
        "        preds = np.vstack(preds)\n",
        "        rmse = np.sqrt(np.mean((y_valid_tensor.numpy() - preds) ** 2))\n",
        "\n",
        "        if rmse < best_rmse:\n",
        "            best_rmse = rmse\n",
        "            trigger_times = 0\n",
        "        else:\n",
        "            trigger_times += 1\n",
        "            if trigger_times >= patience:\n",
        "                break\n",
        "\n",
        "    return best_rmse\n",
        "\n",
        "# âœ… Optuna ì‹¤í–‰\n",
        "study_hparam = optuna.create_study(direction=\"minimize\")\n",
        "study_hparam.optimize(hparam_objective, n_trials=5)\n",
        "\n",
        "print(\"Best trial:\")\n",
        "print(study_hparam.best_trial)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "GwtdQQYQL9nl",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GwtdQQYQL9nl",
        "outputId": "88e0e04e-6647-4839-bf8e-f9485611546d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… ìµœì¢… RMSE: 1,533,396.15\n",
            "âœ… ìµœì¢… RÂ²: 0.67097\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from rtdl import FTTransformer\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# âœ… ê³ ì •ëœ ëª¨ë¸ ë° í•™ìŠµ íŒŒë¼ë¯¸í„°\n",
        "best_model_params = {\n",
        "    \"d_token\": 128,\n",
        "    \"n_blocks\": 5,\n",
        "    \"n_heads\": 2,\n",
        "    \"dropout\": 0.1476045541216017,\n",
        "    \"ffn_d_hidden\": 128\n",
        "}\n",
        "\n",
        "best_hparam = {\n",
        "    \"lr\": 0.001569851841594099,\n",
        "    \"weight_decay\": 0.004428466921695153,\n",
        "    \"batch_size\": 1024\n",
        "}\n",
        "\n",
        "# âœ… ë°ì´í„° ì¤€ë¹„: train + valid í•©ì¹˜ê¸°\n",
        "X_final_tensor = torch.cat([X_train_tensor, X_valid_tensor], dim=0)\n",
        "y_final_tensor = torch.cat([y_train_tensor, y_valid_tensor], dim=0)\n",
        "\n",
        "train_dataset = TensorDataset(X_final_tensor, y_final_tensor)\n",
        "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=best_hparam[\"batch_size\"], shuffle=True, pin_memory=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=best_hparam[\"batch_size\"], shuffle=False, pin_memory=True)\n",
        "\n",
        "# âœ… ëª¨ë¸ ì •ì˜\n",
        "model = FTTransformer.make_baseline(\n",
        "    n_num_features=len(num_indices),\n",
        "    cat_cardinalities=cat_cardinalities,\n",
        "    d_token=best_model_params[\"d_token\"],\n",
        "    n_blocks=best_model_params[\"n_blocks\"],\n",
        "    attention_dropout=best_model_params[\"dropout\"],\n",
        "    ffn_d_hidden=best_model_params[\"ffn_d_hidden\"],\n",
        "    ffn_dropout=best_model_params[\"dropout\"],\n",
        "    residual_dropout=best_model_params[\"dropout\"],\n",
        "    d_out=1\n",
        ").to(device)\n",
        "\n",
        "# âœ… í•™ìŠµ ì¤€ë¹„\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=best_hparam[\"lr\"], weight_decay=best_hparam[\"weight_decay\"])\n",
        "loss_fn = nn.MSELoss()\n",
        "patience = 10\n",
        "trigger_times = 0\n",
        "best_rmse = float(\"inf\")\n",
        "\n",
        "# âœ… í•™ìŠµ ë£¨í”„\n",
        "for epoch in range(80):\n",
        "    model.train()\n",
        "    for xb, yb in train_loader:\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "        x_num, x_cat = split_features(xb)\n",
        "        x_num, x_cat = x_num.to(device), x_cat.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        preds = model(x_num, x_cat)\n",
        "        loss = loss_fn(preds, yb)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# âœ… í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡\n",
        "model.eval()\n",
        "preds = []\n",
        "with torch.no_grad():\n",
        "    for xb, _ in test_loader:\n",
        "        xb = xb.to(device)\n",
        "        x_num, x_cat = split_features(xb)\n",
        "        x_num, x_cat = x_num.to(device), x_cat.to(device)\n",
        "\n",
        "        pred = model(x_num, x_cat)\n",
        "        preds.append(pred.cpu().numpy())\n",
        "\n",
        "preds = np.vstack(preds)\n",
        "y_test = y_test_tensor.numpy()\n",
        "\n",
        "# âœ… ì—­ì •ê·œí™”\n",
        "y_min = 216.0\n",
        "y_max = 165300000.0\n",
        "\n",
        "preds_inverse = preds * (y_max - y_min) + y_min\n",
        "y_test_inverse = y_test * (y_max - y_min) + y_min\n",
        "\n",
        "# âœ… ì„±ëŠ¥ í‰ê°€\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "mse = mean_squared_error(y_test_inverse, preds_inverse)\n",
        "rmse = np.sqrt(mse)\n",
        "r2 = r2_score(y_test_inverse, preds_inverse)\n",
        "\n",
        "print(f\"âœ… ìµœì¢… RMSE: {rmse:,.2f}\")\n",
        "print(f\"âœ… ìµœì¢… RÂ²: {r2:.5f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "s8SJELTwjoAZ",
      "metadata": {
        "id": "s8SJELTwjoAZ"
      },
      "outputs": [],
      "source": [
        "torch.save(model, \"fttransformer_trained.pt\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QN7Xh1v8kjF-",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QN7Xh1v8kjF-",
        "outputId": "9e510044-12fd-4b61-bde3-6d18c2faa8db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[1.3846904e-05]\n",
            " [2.4620444e-05]\n",
            " [2.9076960e-02]\n",
            " ...\n",
            " [6.8318099e-05]\n",
            " [2.1006912e-05]\n",
            " [1.0691740e-02]]\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "# ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°\n",
        "model = torch.load(\"fttransformer_trained.pt\")\n",
        "model.to(device)\n",
        "model.eval()  # í‰ê°€ ëª¨ë“œ ì „í™˜\n",
        "\n",
        "# ì˜ˆì¸¡\n",
        "test_loader = DataLoader(test_dataset, batch_size=1024, shuffle=False)\n",
        "preds = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for xb, _ in test_loader:\n",
        "        xb = xb.to(device)\n",
        "        pred = model(xb, None)  # ìˆ˜ì¹˜í˜•-only ì…ë ¥\n",
        "        preds.append(pred.cpu().numpy())\n",
        "\n",
        "preds = np.vstack(preds)\n",
        "print(preds)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5f0699c",
      "metadata": {
        "id": "e5f0699c"
      },
      "source": [
        "### ì „ì²´ ê²°ê³¼ ì‹œê°í™”"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "1a9cb075",
      "metadata": {
        "id": "1a9cb075",
        "outputId": "22c9681e-9850-4d83-bea2-20805ad5a694"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "             ëª¨ë¸ëª…        RMSE       RÂ²\n",
            "0       CatBoost  1518386.06  0.67740\n",
            "1       LightGBM  1532644.61  0.67130\n",
            "2        XGBoost  1526182.97  0.67410\n",
            "3        HistGBR  1538908.34  0.66860\n",
            "4   RandomForest  1546480.83  0.66530\n",
            "5         TabNet  1568550.27  0.65572\n",
            "6  FTTransformer  1533396.15  0.67097\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# ì£¼ì–´ì§„ ëª¨ë¸ ì„±ëŠ¥ ê²°ê³¼ ê¸°ë°˜ ë°ì´í„°í”„ë ˆì„ ìƒì„±\n",
        "df_results = pd.DataFrame([\n",
        "    {\"ëª¨ë¸ëª…\": \"CatBoost\", \"RMSE\": 1518386.06, \"RÂ²\": 0.6774},\n",
        "    {\"ëª¨ë¸ëª…\": \"LightGBM\", \"RMSE\": 1532644.61, \"RÂ²\": 0.6713},\n",
        "    {\"ëª¨ë¸ëª…\": \"XGBoost\", \"RMSE\": 1526182.97, \"RÂ²\": 0.6741},\n",
        "    {\"ëª¨ë¸ëª…\": \"HistGBR\", \"RMSE\": 1538908.34, \"RÂ²\": 0.6686},\n",
        "    {\"ëª¨ë¸ëª…\": \"RandomForest\", \"RMSE\": 1546480.83, \"RÂ²\": 0.6653}\n",
        "])\n",
        "\n",
        "# TabNet & FTTransformer ê²°ê³¼ ì¶”ê°€\n",
        "tabnet_result = {\"ëª¨ë¸ëª…\": \"TabNet\", \"RMSE\": 1568550.27, \"RÂ²\": 0.65572}\n",
        "fttransformer_result = {\"ëª¨ë¸ëª…\": \"FTTransformer\", \"RMSE\": 1533396.15, \"RÂ²\": 0.67097}\n",
        "\n",
        "df_results = pd.concat([\n",
        "    df_results,\n",
        "    pd.DataFrame([tabnet_result, fttransformer_result])\n",
        "], ignore_index=True)\n",
        "\n",
        "# ë°ì´í„°í”„ë ˆì„ ì¶œë ¥\n",
        "print(df_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "W1T4avz1Um3l",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "W1T4avz1Um3l",
        "outputId": "82ff659e-1643-4308-df2c-6e849f2fa8cf"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1200x500 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt \n",
        "# ìƒ‰ìƒ (7ê°œ ëª¨ë¸)\n",
        "\n",
        "colors = ['#17becf', '#bcbd22', '#7f7f7f', '#aec7e8', '#ffbb78', '#98df8a', '#ff9896']\n",
        "\n",
        "# ì‹œê°í™”\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# âœ… RMSE ê·¸ë˜í”„\n",
        "plt.subplot(1, 2, 1)\n",
        "bars_rmse = plt.bar(df_results[\"ëª¨ë¸ëª…\"], df_results[\"RMSE\"], color=colors)\n",
        "plt.title(\"ëª¨ë¸ë³„ RMSE (ë‹¨ìœ„: ë°±ë§Œ ì›)\")\n",
        "plt.ylabel(\"RMSE\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.ylim(1500000, 1600000)  # yì¶• ë²”ìœ„ ì„¤ì •\n",
        "\n",
        "# RMSE ê°’ í‘œì‹œ (ì‹¤ì œ ê°’, ì½¤ë§ˆ í¬í•¨)\n",
        "for bar in bars_rmse:\n",
        "    yval = bar.get_height()\n",
        "    plt.text(bar.get_x() + bar.get_width()/2, yval + 5000, f\"{yval:,.0f}\",\n",
        "             ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "# âœ… RÂ² ê·¸ë˜í”„\n",
        "plt.subplot(1, 2, 2)\n",
        "bars_r2 = plt.bar(df_results[\"ëª¨ë¸ëª…\"], df_results[\"RÂ²\"], color=colors)\n",
        "plt.title(\"ëª¨ë¸ë³„ RÂ²\")\n",
        "plt.ylabel(\"RÂ²\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.ylim(0.64, 0.69)  # RÂ² ë²”ìœ„ ì„¤ì • (ì°¨ì´ ê°•ì¡°)\n",
        "\n",
        "# RÂ² ê°’ í‘œì‹œ\n",
        "for bar in bars_r2:\n",
        "    yval = bar.get_height()\n",
        "    plt.text(bar.get_x() + bar.get_width()/2, yval + 0.001, f\"{yval:.4f}\",\n",
        "             ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zme04tNDUyBR",
      "metadata": {
        "id": "zme04tNDUyBR"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
